Mon Mar  6 20:41:09 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.11    Driver Version: 525.60.11    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |
| N/A   41C    P8     9W /  70W |      2MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
 20:41:09 up 5 days,  6:55,  1 user,  load average: 0.50, 0.70, 0.74
Folder of Checkpoints: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted
Test File: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Config: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/configs/RoBERTa_config_fast.json
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Testing checkpoints found in /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted
10 checkpoints found
Testing...
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=0-step=206.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]  0%|          | 1/210 [00:02<08:26,  2.42s/it]  1%|▏         | 3/210 [00:02<02:23,  1.44it/s]  2%|▏         | 5/210 [00:02<01:14,  2.74it/s]  3%|▎         | 7/210 [00:02<00:47,  4.28it/s]  4%|▍         | 9/210 [00:02<00:33,  6.07it/s]  5%|▌         | 11/210 [00:03<00:25,  7.66it/s]  6%|▌         | 13/210 [00:03<00:21,  9.00it/s]  7%|▋         | 15/210 [00:03<00:20,  9.61it/s]  8%|▊         | 17/210 [00:03<00:17, 11.12it/s]  9%|▉         | 19/210 [00:03<00:15, 12.65it/s] 10%|█         | 21/210 [00:03<00:13, 13.90it/s] 11%|█         | 23/210 [00:03<00:12, 15.15it/s] 12%|█▏        | 26/210 [00:03<00:11, 16.62it/s] 14%|█▍        | 29/210 [00:04<00:10, 17.59it/s] 15%|█▍        | 31/210 [00:04<00:10, 16.67it/s] 16%|█▌        | 34/210 [00:04<00:09, 18.33it/s] 17%|█▋        | 36/210 [00:04<00:09, 17.49it/s] 18%|█▊        | 38/210 [00:04<00:09, 17.24it/s] 19%|█▉        | 40/210 [00:04<00:10, 16.64it/s] 20%|██        | 42/210 [00:04<00:10, 15.88it/s] 21%|██▏       | 45/210 [00:05<00:09, 17.63it/s] 23%|██▎       | 48/210 [00:05<00:08, 18.88it/s] 24%|██▍       | 51/210 [00:05<00:08, 19.35it/s] 26%|██▌       | 54/210 [00:05<00:07, 19.81it/s] 27%|██▋       | 56/210 [00:05<00:08, 18.65it/s] 28%|██▊       | 58/210 [00:05<00:08, 18.23it/s] 29%|██▊       | 60/210 [00:05<00:08, 18.54it/s] 30%|██▉       | 62/210 [00:05<00:08, 17.34it/s] 30%|███       | 64/210 [00:06<00:08, 17.80it/s] 32%|███▏      | 67/210 [00:06<00:08, 17.55it/s] 33%|███▎      | 69/210 [00:06<00:08, 16.89it/s] 34%|███▍      | 71/210 [00:06<00:08, 17.31it/s] 35%|███▌      | 74/210 [00:06<00:07, 18.00it/s] 36%|███▌      | 76/210 [00:06<00:07, 17.36it/s] 37%|███▋      | 78/210 [00:06<00:08, 15.64it/s] 39%|███▊      | 81/210 [00:07<00:07, 17.33it/s] 40%|████      | 84/210 [00:07<00:07, 17.96it/s] 41%|████▏     | 87/210 [00:07<00:06, 18.60it/s] 42%|████▏     | 89/210 [00:07<00:06, 18.16it/s] 43%|████▎     | 91/210 [00:07<00:06, 18.22it/s] 44%|████▍     | 93/210 [00:07<00:06, 17.99it/s] 46%|████▌     | 96/210 [00:07<00:05, 19.90it/s] 47%|████▋     | 99/210 [00:08<00:05, 19.53it/s] 49%|████▊     | 102/210 [00:08<00:05, 21.22it/s] 50%|█████     | 105/210 [00:08<00:05, 20.29it/s] 51%|█████▏    | 108/210 [00:08<00:05, 19.41it/s] 52%|█████▏    | 110/210 [00:08<00:05, 18.05it/s] 54%|█████▍    | 113/210 [00:08<00:05, 19.21it/s] 55%|█████▍    | 115/210 [00:08<00:05, 18.29it/s] 56%|█████▌    | 117/210 [00:08<00:05, 18.48it/s] 57%|█████▋    | 120/210 [00:09<00:05, 17.99it/s] 58%|█████▊    | 122/210 [00:09<00:04, 18.43it/s] 59%|█████▉    | 124/210 [00:09<00:04, 17.76it/s] 60%|██████    | 126/210 [00:09<00:04, 17.99it/s] 61%|██████    | 128/210 [00:09<00:04, 17.51it/s] 62%|██████▏   | 130/210 [00:09<00:04, 17.05it/s] 63%|██████▎   | 132/210 [00:09<00:04, 17.43it/s] 64%|██████▍   | 134/210 [00:09<00:04, 17.42it/s] 65%|██████▍   | 136/210 [00:10<00:04, 17.97it/s] 66%|██████▌   | 138/210 [00:10<00:04, 17.02it/s] 67%|██████▋   | 140/210 [00:10<00:03, 17.67it/s] 68%|██████▊   | 142/210 [00:10<00:04, 16.25it/s] 69%|██████▊   | 144/210 [00:10<00:04, 14.86it/s] 70%|██████▉   | 146/210 [00:10<00:04, 15.52it/s] 70%|███████   | 148/210 [00:10<00:03, 15.78it/s] 71%|███████▏  | 150/210 [00:11<00:05, 11.92it/s] 72%|███████▏  | 152/210 [00:11<00:04, 12.65it/s] 74%|███████▍  | 155/210 [00:11<00:03, 14.76it/s] 75%|███████▍  | 157/210 [00:11<00:03, 15.65it/s] 76%|███████▌  | 160/210 [00:11<00:02, 16.87it/s] 77%|███████▋  | 162/210 [00:11<00:02, 16.44it/s] 78%|███████▊  | 164/210 [00:11<00:02, 16.74it/s] 79%|███████▉  | 166/210 [00:11<00:02, 16.98it/s] 80%|████████  | 168/210 [00:12<00:02, 16.26it/s] 81%|████████▏ | 171/210 [00:12<00:02, 17.22it/s] 83%|████████▎ | 174/210 [00:12<00:02, 17.51it/s] 84%|████████▍ | 177/210 [00:12<00:01, 18.00it/s] 85%|████████▌ | 179/210 [00:12<00:01, 17.35it/s] 86%|████████▌ | 181/210 [00:12<00:01, 17.73it/s] 88%|████████▊ | 184/210 [00:13<00:01, 17.63it/s] 89%|████████▊ | 186/210 [00:13<00:01, 17.74it/s] 90%|████████▉ | 188/210 [00:13<00:01, 17.43it/s] 91%|█████████ | 191/210 [00:13<00:00, 19.04it/s] 92%|█████████▏| 193/210 [00:13<00:00, 18.20it/s] 93%|█████████▎| 195/210 [00:13<00:00, 17.60it/s] 94%|█████████▍| 197/210 [00:13<00:00, 18.03it/s] 95%|█████████▍| 199/210 [00:13<00:00, 16.41it/s] 96%|█████████▌| 202/210 [00:14<00:00, 18.39it/s] 97%|█████████▋| 204/210 [00:14<00:00, 18.32it/s] 98%|█████████▊| 206/210 [00:14<00:00, 18.27it/s] 99%|█████████▉| 208/210 [00:14<00:00, 15.98it/s]100%|██████████| 210/210 [00:14<00:00, 14.45it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1763  149]
 [  53  128]]
tn=1763, fp=149, fn=53, tp=128
Number of predicted 0s: 1816
Number of actual 0s: 1912
Number of predicted 1s: 277
Number of actual 1s: 181
Precision: 0.4620938628158845
Accuracy: 0.9034878165312948
Recall: 0.7071823204419889
F1 score: 0.5589519650655023
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=1-step=412.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:37,  2.13it/s]  1%|▏         | 3/210 [00:00<00:41,  4.96it/s]  2%|▏         | 5/210 [00:00<00:26,  7.79it/s]  3%|▎         | 7/210 [00:00<00:19, 10.37it/s]  4%|▍         | 9/210 [00:01<00:16, 12.50it/s]  5%|▌         | 11/210 [00:01<00:15, 13.16it/s]  6%|▌         | 13/210 [00:01<00:14, 13.33it/s]  7%|▋         | 15/210 [00:01<00:15, 12.52it/s]  8%|▊         | 17/210 [00:01<00:14, 13.53it/s]  9%|▉         | 19/210 [00:01<00:13, 14.39it/s] 10%|█         | 21/210 [00:01<00:12, 15.28it/s] 11%|█         | 23/210 [00:01<00:11, 16.16it/s] 12%|█▏        | 26/210 [00:02<00:10, 17.28it/s] 14%|█▍        | 29/210 [00:02<00:10, 17.90it/s] 15%|█▍        | 31/210 [00:02<00:10, 16.82it/s] 16%|█▌        | 34/210 [00:02<00:09, 18.42it/s] 17%|█▋        | 36/210 [00:02<00:09, 17.47it/s] 18%|█▊        | 38/210 [00:02<00:09, 17.25it/s] 19%|█▉        | 40/210 [00:02<00:10, 16.58it/s] 20%|██        | 42/210 [00:03<00:10, 15.82it/s] 21%|██▏       | 45/210 [00:03<00:09, 17.46it/s] 23%|██▎       | 48/210 [00:03<00:08, 18.66it/s] 24%|██▍       | 51/210 [00:03<00:08, 19.01it/s] 26%|██▌       | 54/210 [00:03<00:08, 19.36it/s] 27%|██▋       | 56/210 [00:03<00:08, 18.18it/s] 28%|██▊       | 58/210 [00:03<00:08, 17.73it/s] 29%|██▊       | 60/210 [00:03<00:08, 18.03it/s] 30%|██▉       | 62/210 [00:04<00:08, 16.95it/s] 30%|███       | 64/210 [00:04<00:08, 17.41it/s] 31%|███▏      | 66/210 [00:04<00:07, 18.06it/s] 32%|███▏      | 68/210 [00:04<00:08, 16.11it/s] 33%|███▎      | 70/210 [00:04<00:08, 16.30it/s] 34%|███▍      | 72/210 [00:04<00:08, 17.07it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.62it/s] 36%|███▌      | 76/210 [00:04<00:07, 16.84it/s] 37%|███▋      | 78/210 [00:05<00:08, 15.12it/s] 39%|███▊      | 81/210 [00:05<00:07, 17.04it/s] 40%|████      | 84/210 [00:05<00:07, 17.76it/s] 41%|████▏     | 87/210 [00:05<00:06, 18.34it/s] 42%|████▏     | 89/210 [00:05<00:06, 17.92it/s] 43%|████▎     | 91/210 [00:05<00:06, 18.39it/s] 44%|████▍     | 93/210 [00:05<00:06, 17.84it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.69it/s] 47%|████▋     | 99/210 [00:06<00:05, 19.08it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.88it/s] 50%|█████     | 105/210 [00:06<00:05, 20.01it/s] 51%|█████▏    | 108/210 [00:06<00:05, 19.30it/s] 52%|█████▏    | 110/210 [00:06<00:05, 17.79it/s] 54%|█████▍    | 113/210 [00:06<00:05, 18.84it/s] 55%|█████▍    | 115/210 [00:07<00:05, 18.00it/s] 56%|█████▌    | 117/210 [00:07<00:05, 18.18it/s] 57%|█████▋    | 120/210 [00:07<00:05, 17.54it/s] 58%|█████▊    | 122/210 [00:07<00:04, 18.04it/s] 59%|█████▉    | 124/210 [00:07<00:04, 17.59it/s] 60%|██████    | 126/210 [00:07<00:04, 18.04it/s] 61%|██████    | 128/210 [00:07<00:04, 17.62it/s] 62%|██████▏   | 130/210 [00:07<00:04, 17.16it/s] 63%|██████▎   | 132/210 [00:08<00:04, 17.51it/s] 64%|██████▍   | 134/210 [00:08<00:04, 17.32it/s] 65%|██████▍   | 136/210 [00:08<00:04, 17.73it/s] 66%|██████▌   | 138/210 [00:08<00:04, 16.56it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.95it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.71it/s] 69%|██████▊   | 144/210 [00:08<00:04, 14.41it/s] 70%|██████▉   | 146/210 [00:08<00:04, 15.19it/s] 70%|███████   | 148/210 [00:09<00:03, 15.59it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.85it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.57it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.56it/s] 75%|███████▍  | 157/210 [00:09<00:03, 15.41it/s] 76%|███████▌  | 159/210 [00:09<00:03, 16.35it/s] 77%|███████▋  | 161/210 [00:09<00:02, 16.65it/s] 78%|███████▊  | 163/210 [00:10<00:03, 15.59it/s] 79%|███████▊  | 165/210 [00:10<00:02, 16.46it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.90it/s] 80%|████████  | 169/210 [00:10<00:02, 16.26it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.98it/s] 83%|████████▎ | 174/210 [00:10<00:02, 17.19it/s] 84%|████████▍ | 177/210 [00:10<00:01, 17.61it/s] 85%|████████▌ | 179/210 [00:11<00:01, 17.04it/s] 86%|████████▌ | 181/210 [00:11<00:01, 17.43it/s] 88%|████████▊ | 184/210 [00:11<00:01, 17.21it/s] 89%|████████▊ | 186/210 [00:11<00:01, 17.32it/s] 90%|████████▉ | 188/210 [00:11<00:01, 17.05it/s] 91%|█████████ | 191/210 [00:11<00:01, 18.64it/s] 92%|█████████▏| 193/210 [00:11<00:00, 17.77it/s] 93%|█████████▎| 195/210 [00:11<00:00, 17.03it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.37it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.90it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.81it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.86it/s] 98%|█████████▊| 206/210 [00:12<00:00, 17.74it/s] 99%|█████████▉| 208/210 [00:12<00:00, 15.54it/s]100%|██████████| 210/210 [00:12<00:00, 16.30it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1710  202]
 [  39  142]]
tn=1710, fp=202, fn=39, tp=142
Number of predicted 0s: 1749
Number of actual 0s: 1912
Number of predicted 1s: 344
Number of actual 1s: 181
Precision: 0.4127906976744186
Accuracy: 0.884854276158624
Recall: 0.7845303867403315
F1 score: 0.5409523809523809
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=2-step=618.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:39,  2.10it/s]  1%|▏         | 3/210 [00:00<00:39,  5.21it/s]  2%|▏         | 5/210 [00:00<00:25,  8.11it/s]  3%|▎         | 7/210 [00:00<00:19, 10.54it/s]  4%|▍         | 9/210 [00:00<00:15, 12.62it/s]  5%|▌         | 11/210 [00:01<00:14, 13.30it/s]  6%|▌         | 13/210 [00:01<00:14, 13.34it/s]  7%|▋         | 15/210 [00:01<00:15, 12.36it/s]  8%|▊         | 17/210 [00:01<00:14, 13.42it/s]  9%|▉         | 19/210 [00:01<00:13, 14.34it/s] 10%|█         | 21/210 [00:01<00:12, 15.20it/s] 11%|█         | 23/210 [00:01<00:11, 16.05it/s] 12%|█▏        | 26/210 [00:02<00:10, 17.04it/s] 14%|█▍        | 29/210 [00:02<00:10, 17.77it/s] 15%|█▍        | 31/210 [00:02<00:10, 16.73it/s] 16%|█▌        | 34/210 [00:02<00:09, 18.29it/s] 17%|█▋        | 36/210 [00:02<00:10, 17.20it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.92it/s] 19%|█▉        | 40/210 [00:02<00:10, 16.23it/s] 20%|██        | 42/210 [00:03<00:10, 15.49it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.98it/s] 23%|██▎       | 48/210 [00:03<00:08, 18.29it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.78it/s] 26%|██▌       | 54/210 [00:03<00:08, 19.11it/s] 27%|██▋       | 56/210 [00:03<00:08, 18.01it/s] 28%|██▊       | 58/210 [00:03<00:08, 17.62it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.89it/s] 30%|██▉       | 62/210 [00:04<00:08, 16.80it/s] 30%|███       | 64/210 [00:04<00:08, 17.38it/s] 32%|███▏      | 67/210 [00:04<00:08, 17.21it/s] 33%|███▎      | 69/210 [00:04<00:08, 16.59it/s] 34%|███▍      | 71/210 [00:04<00:08, 16.85it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.52it/s] 36%|███▌      | 76/210 [00:04<00:07, 16.84it/s] 37%|███▋      | 78/210 [00:05<00:08, 15.19it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.70it/s] 40%|████      | 84/210 [00:05<00:07, 17.41it/s] 41%|████▏     | 87/210 [00:05<00:06, 18.12it/s] 42%|████▏     | 89/210 [00:05<00:06, 17.75it/s] 43%|████▎     | 91/210 [00:05<00:06, 18.22it/s] 44%|████▍     | 93/210 [00:05<00:06, 17.82it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.57it/s] 47%|████▋     | 99/210 [00:06<00:05, 18.96it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.58it/s] 50%|█████     | 105/210 [00:06<00:05, 19.74it/s] 51%|█████▏    | 108/210 [00:06<00:05, 19.14it/s] 52%|█████▏    | 110/210 [00:06<00:05, 17.81it/s] 54%|█████▍    | 113/210 [00:06<00:05, 18.93it/s] 55%|█████▍    | 115/210 [00:07<00:05, 18.09it/s] 56%|█████▌    | 117/210 [00:07<00:05, 18.27it/s] 57%|█████▋    | 120/210 [00:07<00:05, 17.56it/s] 58%|█████▊    | 122/210 [00:07<00:04, 18.06it/s] 59%|█████▉    | 124/210 [00:07<00:04, 17.56it/s] 60%|██████    | 126/210 [00:07<00:04, 17.92it/s] 61%|██████    | 128/210 [00:07<00:04, 17.33it/s] 62%|██████▏   | 130/210 [00:07<00:04, 16.84it/s] 63%|██████▎   | 132/210 [00:08<00:04, 17.14it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.96it/s] 65%|██████▍   | 136/210 [00:08<00:04, 17.40it/s] 66%|██████▌   | 138/210 [00:08<00:04, 16.45it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.92it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.72it/s] 69%|██████▊   | 144/210 [00:08<00:04, 14.47it/s] 70%|██████▉   | 146/210 [00:08<00:04, 15.15it/s] 70%|███████   | 148/210 [00:09<00:04, 15.48it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.64it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.29it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.38it/s] 75%|███████▍  | 157/210 [00:09<00:03, 15.26it/s] 76%|███████▌  | 159/210 [00:09<00:03, 16.27it/s] 77%|███████▋  | 161/210 [00:09<00:02, 16.49it/s] 78%|███████▊  | 163/210 [00:10<00:03, 15.43it/s] 79%|███████▊  | 165/210 [00:10<00:02, 16.29it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.68it/s] 80%|████████  | 169/210 [00:10<00:02, 16.00it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.73it/s] 83%|████████▎ | 174/210 [00:10<00:02, 17.08it/s] 84%|████████▍ | 177/210 [00:10<00:01, 17.74it/s] 85%|████████▌ | 179/210 [00:11<00:01, 17.11it/s] 86%|████████▌ | 181/210 [00:11<00:01, 17.46it/s] 88%|████████▊ | 184/210 [00:11<00:01, 17.18it/s] 89%|████████▊ | 186/210 [00:11<00:01, 17.31it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.97it/s] 91%|█████████ | 191/210 [00:11<00:01, 18.45it/s] 92%|█████████▏| 193/210 [00:11<00:00, 17.72it/s] 93%|█████████▎| 195/210 [00:11<00:00, 17.10it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.49it/s] 95%|█████████▍| 199/210 [00:12<00:00, 16.00it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.85it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.88it/s] 98%|█████████▊| 206/210 [00:12<00:00, 17.74it/s] 99%|█████████▉| 208/210 [00:12<00:00, 15.43it/s]100%|██████████| 210/210 [00:12<00:00, 16.22it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1836   76]
 [  73  108]]
tn=1836, fp=76, fn=73, tp=108
Number of predicted 0s: 1909
Number of actual 0s: 1912
Number of predicted 1s: 184
Number of actual 1s: 181
Precision: 0.5869565217391305
Accuracy: 0.9288103201146679
Recall: 0.5966850828729282
F1 score: 0.5917808219178081
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=3-step=824.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:34,  2.20it/s]  1%|▏         | 3/210 [00:00<00:38,  5.34it/s]  2%|▏         | 5/210 [00:00<00:24,  8.23it/s]  3%|▎         | 7/210 [00:00<00:19, 10.57it/s]  4%|▍         | 9/210 [00:00<00:15, 12.62it/s]  5%|▌         | 11/210 [00:01<00:15, 13.18it/s]  6%|▌         | 13/210 [00:01<00:14, 13.27it/s]  7%|▋         | 15/210 [00:01<00:15, 12.41it/s]  8%|▊         | 17/210 [00:01<00:14, 13.57it/s]  9%|▉         | 19/210 [00:01<00:13, 14.33it/s] 10%|█         | 21/210 [00:01<00:12, 15.13it/s] 11%|█         | 23/210 [00:01<00:11, 16.07it/s] 12%|█▏        | 26/210 [00:02<00:10, 17.10it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.70it/s] 14%|█▍        | 30/210 [00:02<00:10, 16.56it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.86it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.86it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.68it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.58it/s] 20%|█▉        | 41/210 [00:02<00:10, 16.45it/s] 20%|██        | 43/210 [00:03<00:10, 16.00it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.98it/s] 23%|██▎       | 48/210 [00:03<00:08, 18.20it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.71it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.93it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.75it/s] 28%|██▊       | 58/210 [00:03<00:08, 17.32it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.69it/s] 30%|██▉       | 62/210 [00:04<00:08, 16.69it/s] 30%|███       | 64/210 [00:04<00:08, 17.30it/s] 32%|███▏      | 67/210 [00:04<00:08, 17.14it/s] 33%|███▎      | 69/210 [00:04<00:08, 16.50it/s] 34%|███▍      | 71/210 [00:04<00:08, 16.78it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.51it/s] 36%|███▌      | 76/210 [00:04<00:08, 16.66it/s] 37%|███▋      | 78/210 [00:05<00:08, 14.95it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.63it/s] 40%|███▉      | 83/210 [00:05<00:07, 17.38it/s] 40%|████      | 85/210 [00:05<00:07, 17.84it/s] 41%|████▏     | 87/210 [00:05<00:06, 17.88it/s] 42%|████▏     | 89/210 [00:05<00:06, 17.40it/s] 43%|████▎     | 91/210 [00:05<00:06, 17.98it/s] 44%|████▍     | 93/210 [00:05<00:06, 17.54it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.51it/s] 47%|████▋     | 99/210 [00:06<00:05, 19.02it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.77it/s] 50%|█████     | 105/210 [00:06<00:05, 19.89it/s] 51%|█████▏    | 108/210 [00:06<00:05, 19.23it/s] 52%|█████▏    | 110/210 [00:06<00:05, 17.69it/s] 54%|█████▍    | 113/210 [00:06<00:05, 18.76it/s] 55%|█████▍    | 115/210 [00:07<00:05, 17.85it/s] 56%|█████▌    | 117/210 [00:07<00:05, 17.97it/s] 57%|█████▋    | 120/210 [00:07<00:05, 17.30it/s] 58%|█████▊    | 122/210 [00:07<00:04, 17.71it/s] 59%|█████▉    | 124/210 [00:07<00:05, 17.19it/s] 60%|██████    | 126/210 [00:07<00:04, 17.63it/s] 61%|██████    | 128/210 [00:07<00:04, 17.24it/s] 62%|██████▏   | 130/210 [00:07<00:04, 16.85it/s] 63%|██████▎   | 132/210 [00:08<00:04, 17.05it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.90it/s] 65%|██████▍   | 136/210 [00:08<00:04, 17.46it/s] 66%|██████▌   | 138/210 [00:08<00:04, 16.53it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.98it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.73it/s] 69%|██████▊   | 144/210 [00:08<00:04, 14.39it/s] 70%|██████▉   | 146/210 [00:09<00:04, 15.14it/s] 70%|███████   | 148/210 [00:09<00:04, 15.35it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.49it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.18it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.25it/s] 75%|███████▍  | 157/210 [00:09<00:03, 15.20it/s] 76%|███████▌  | 159/210 [00:09<00:03, 16.17it/s] 77%|███████▋  | 161/210 [00:10<00:02, 16.34it/s] 78%|███████▊  | 163/210 [00:10<00:03, 15.24it/s] 79%|███████▊  | 165/210 [00:10<00:02, 16.02it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.39it/s] 80%|████████  | 169/210 [00:10<00:02, 15.89it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.61it/s] 83%|████████▎ | 174/210 [00:10<00:02, 17.01it/s] 84%|████████▍ | 177/210 [00:10<00:01, 17.63it/s] 85%|████████▌ | 179/210 [00:11<00:01, 17.07it/s] 86%|████████▌ | 181/210 [00:11<00:01, 17.39it/s] 88%|████████▊ | 184/210 [00:11<00:01, 17.15it/s] 89%|████████▊ | 186/210 [00:11<00:01, 17.13it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.73it/s] 91%|█████████ | 191/210 [00:11<00:01, 18.31it/s] 92%|█████████▏| 193/210 [00:11<00:00, 17.52it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.86it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.16it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.67it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.62it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.47it/s] 98%|█████████▊| 206/210 [00:12<00:00, 17.52it/s] 99%|█████████▉| 208/210 [00:12<00:00, 15.28it/s]100%|██████████| 210/210 [00:13<00:00, 16.12it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1866   46]
 [ 104   77]]
tn=1866, fp=46, fn=104, tp=77
Number of predicted 0s: 1970
Number of actual 0s: 1912
Number of predicted 1s: 123
Number of actual 1s: 181
Precision: 0.6260162601626016
Accuracy: 0.9283325370281892
Recall: 0.425414364640884
F1 score: 0.5065789473684209
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=4-step=1030.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:33,  2.22it/s]  1%|▏         | 3/210 [00:00<00:39,  5.28it/s]  2%|▏         | 5/210 [00:00<00:25,  8.16it/s]  3%|▎         | 7/210 [00:00<00:19, 10.58it/s]  4%|▍         | 9/210 [00:00<00:15, 12.63it/s]  5%|▌         | 11/210 [00:01<00:15, 13.18it/s]  6%|▌         | 13/210 [00:01<00:14, 13.19it/s]  7%|▋         | 15/210 [00:01<00:15, 12.23it/s]  8%|▊         | 17/210 [00:01<00:14, 13.29it/s]  9%|▉         | 19/210 [00:01<00:13, 14.10it/s] 10%|█         | 21/210 [00:01<00:12, 14.99it/s] 11%|█         | 23/210 [00:01<00:11, 15.89it/s] 12%|█▏        | 26/210 [00:02<00:10, 16.81it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.42it/s] 14%|█▍        | 30/210 [00:02<00:10, 16.44it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.71it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.49it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.45it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.32it/s] 20%|█▉        | 41/210 [00:02<00:10, 16.16it/s] 20%|██        | 43/210 [00:03<00:10, 15.77it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.71it/s] 23%|██▎       | 48/210 [00:03<00:09, 18.00it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.45it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.78it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.65it/s] 28%|██▊       | 58/210 [00:03<00:08, 17.27it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.71it/s] 30%|██▉       | 62/210 [00:04<00:08, 16.62it/s] 30%|███       | 64/210 [00:04<00:08, 17.05it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.71it/s] 32%|███▏      | 68/210 [00:04<00:08, 15.84it/s] 33%|███▎      | 70/210 [00:04<00:08, 15.98it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.72it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.36it/s] 36%|███▌      | 76/210 [00:05<00:08, 16.51it/s] 37%|███▋      | 78/210 [00:05<00:08, 14.70it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.52it/s] 40%|███▉      | 83/210 [00:05<00:07, 17.17it/s] 40%|████      | 85/210 [00:05<00:07, 17.61it/s] 41%|████▏     | 87/210 [00:05<00:06, 17.66it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.17it/s] 43%|████▎     | 91/210 [00:05<00:06, 17.59it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.32it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.28it/s] 47%|████▋     | 99/210 [00:06<00:05, 18.82it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.63it/s] 50%|█████     | 105/210 [00:06<00:05, 19.72it/s] 51%|█████▏    | 108/210 [00:06<00:05, 19.14it/s] 52%|█████▏    | 110/210 [00:06<00:05, 17.57it/s] 54%|█████▍    | 113/210 [00:07<00:05, 18.58it/s] 55%|█████▍    | 115/210 [00:07<00:05, 17.70it/s] 56%|█████▌    | 117/210 [00:07<00:05, 17.86it/s] 57%|█████▋    | 120/210 [00:07<00:05, 17.23it/s] 58%|█████▊    | 122/210 [00:07<00:04, 17.66it/s] 59%|█████▉    | 124/210 [00:07<00:05, 17.17it/s] 60%|██████    | 126/210 [00:07<00:04, 17.64it/s] 61%|██████    | 128/210 [00:07<00:04, 17.20it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.67it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.87it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.65it/s] 65%|██████▍   | 136/210 [00:08<00:04, 17.13it/s] 66%|██████▌   | 138/210 [00:08<00:04, 16.26it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.70it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.54it/s] 69%|██████▊   | 144/210 [00:08<00:04, 14.22it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.89it/s] 70%|███████   | 148/210 [00:09<00:04, 15.20it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.49it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.13it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.17it/s] 75%|███████▍  | 157/210 [00:09<00:03, 15.06it/s] 76%|███████▌  | 159/210 [00:10<00:03, 16.08it/s] 77%|███████▋  | 161/210 [00:10<00:03, 16.28it/s] 78%|███████▊  | 163/210 [00:10<00:03, 15.27it/s] 79%|███████▊  | 165/210 [00:10<00:02, 16.04it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.51it/s] 80%|████████  | 169/210 [00:10<00:02, 15.98it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.68it/s] 83%|████████▎ | 174/210 [00:10<00:02, 17.00it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.56it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.77it/s] 86%|████████▌ | 181/210 [00:11<00:01, 17.18it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.83it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.85it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.50it/s] 91%|█████████ | 191/210 [00:11<00:01, 18.15it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.42it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.72it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.11it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.64it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.53it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.50it/s] 98%|█████████▊| 206/210 [00:12<00:00, 17.47it/s] 99%|█████████▉| 208/210 [00:12<00:00, 15.32it/s]100%|██████████| 210/210 [00:13<00:00, 16.00it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1869   43]
 [ 109   72]]
tn=1869, fp=43, fn=109, tp=72
Number of predicted 0s: 1978
Number of actual 0s: 1912
Number of predicted 1s: 115
Number of actual 1s: 181
Precision: 0.6260869565217392
Accuracy: 0.9273769708552317
Recall: 0.39779005524861877
F1 score: 0.4864864864864865
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=5-step=1236.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:48,  1.94it/s]  1%|▏         | 3/210 [00:00<00:42,  4.92it/s]  2%|▏         | 5/210 [00:00<00:26,  7.78it/s]  3%|▎         | 7/210 [00:00<00:20, 10.10it/s]  4%|▍         | 9/210 [00:01<00:16, 12.17it/s]  5%|▌         | 11/210 [00:01<00:15, 12.82it/s]  6%|▌         | 13/210 [00:01<00:15, 12.99it/s]  7%|▋         | 15/210 [00:01<00:16, 12.09it/s]  8%|▊         | 17/210 [00:01<00:14, 13.27it/s]  9%|▉         | 19/210 [00:01<00:13, 14.20it/s] 10%|█         | 21/210 [00:01<00:12, 15.00it/s] 11%|█         | 23/210 [00:01<00:11, 15.89it/s] 12%|█▏        | 26/210 [00:02<00:10, 16.78it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.41it/s] 14%|█▍        | 30/210 [00:02<00:10, 16.43it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.78it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.63it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.55it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.37it/s] 20%|█▉        | 41/210 [00:03<00:10, 16.19it/s] 20%|██        | 43/210 [00:03<00:10, 15.69it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.57it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.88it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.32it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.81it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.63it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.28it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.73it/s] 30%|██▉       | 62/210 [00:04<00:08, 16.57it/s] 30%|███       | 64/210 [00:04<00:08, 16.94it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.51it/s] 32%|███▏      | 68/210 [00:04<00:09, 15.70it/s] 33%|███▎      | 70/210 [00:04<00:08, 15.90it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.61it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.29it/s] 36%|███▌      | 76/210 [00:05<00:08, 16.46it/s] 37%|███▋      | 78/210 [00:05<00:08, 14.72it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.17it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.99it/s] 40%|████      | 85/210 [00:05<00:07, 17.51it/s] 41%|████▏     | 87/210 [00:05<00:06, 17.58it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.13it/s] 43%|████▎     | 91/210 [00:05<00:06, 17.76it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.34it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.13it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.45it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.15it/s] 50%|█████     | 105/210 [00:06<00:05, 19.38it/s] 51%|█████     | 107/210 [00:06<00:05, 18.57it/s] 52%|█████▏    | 109/210 [00:06<00:05, 18.25it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.69it/s] 54%|█████▍    | 114/210 [00:07<00:05, 18.00it/s] 55%|█████▌    | 116/210 [00:07<00:05, 18.06it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.81it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.85it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.42it/s] 59%|█████▉    | 124/210 [00:07<00:05, 17.01it/s] 60%|██████    | 126/210 [00:07<00:04, 17.47it/s] 61%|██████    | 128/210 [00:08<00:04, 16.85it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.40it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.67it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.57it/s] 65%|██████▍   | 136/210 [00:08<00:04, 17.16it/s] 66%|██████▌   | 138/210 [00:08<00:04, 16.06it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.67it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.36it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.21it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.85it/s] 70%|███████   | 148/210 [00:09<00:04, 14.93it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.31it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.00it/s] 74%|███████▍  | 155/210 [00:09<00:03, 13.92it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.80it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.83it/s] 77%|███████▋  | 161/210 [00:10<00:03, 16.18it/s] 78%|███████▊  | 163/210 [00:10<00:03, 15.23it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.90it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.33it/s] 80%|████████  | 169/210 [00:10<00:02, 15.76it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.39it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.72it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.25it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.58it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.95it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.76it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.95it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.47it/s] 91%|█████████ | 191/210 [00:12<00:01, 18.11it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.28it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.56it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.92it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.53it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.44it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.48it/s] 98%|█████████▊| 206/210 [00:12<00:00, 17.42it/s] 99%|█████████▉| 208/210 [00:13<00:00, 15.20it/s]100%|██████████| 210/210 [00:13<00:00, 15.82it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1842   70]
 [  77  104]]
tn=1842, fp=70, fn=77, tp=104
Number of predicted 0s: 1919
Number of actual 0s: 1912
Number of predicted 1s: 174
Number of actual 1s: 181
Precision: 0.5977011494252874
Accuracy: 0.9297658862876255
Recall: 0.574585635359116
F1 score: 0.5859154929577465
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=6-step=1442.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:45,  1.97it/s]  1%|▏         | 3/210 [00:00<00:43,  4.78it/s]  2%|▏         | 5/210 [00:00<00:26,  7.61it/s]  3%|▎         | 7/210 [00:00<00:20,  9.97it/s]  4%|▍         | 9/210 [00:01<00:16, 12.01it/s]  5%|▌         | 11/210 [00:01<00:15, 12.69it/s]  6%|▌         | 13/210 [00:01<00:15, 12.85it/s]  7%|▋         | 15/210 [00:01<00:16, 11.90it/s]  8%|▊         | 17/210 [00:01<00:14, 12.90it/s]  9%|▉         | 19/210 [00:01<00:13, 13.80it/s] 10%|█         | 21/210 [00:01<00:12, 14.71it/s] 11%|█         | 23/210 [00:02<00:12, 15.57it/s] 12%|█▏        | 26/210 [00:02<00:11, 16.61it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.25it/s] 14%|█▍        | 30/210 [00:02<00:11, 16.35it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.73it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.56it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.57it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.41it/s] 20%|█▉        | 41/210 [00:03<00:10, 16.25it/s] 20%|██        | 43/210 [00:03<00:10, 15.74it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.54it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.88it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.35it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.76it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.47it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.00it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.34it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.29it/s] 30%|███       | 64/210 [00:04<00:08, 16.71it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.35it/s] 32%|███▏      | 68/210 [00:04<00:09, 15.61it/s] 33%|███▎      | 70/210 [00:04<00:08, 15.80it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.55it/s] 35%|███▌      | 74/210 [00:05<00:07, 17.21it/s] 36%|███▌      | 76/210 [00:05<00:08, 16.35it/s] 37%|███▋      | 78/210 [00:05<00:08, 14.69it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.17it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.93it/s] 40%|████      | 85/210 [00:05<00:07, 17.36it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.44it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.05it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.41it/s] 44%|████▍     | 93/210 [00:06<00:06, 16.96it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.90it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.24it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.02it/s] 50%|█████     | 105/210 [00:06<00:05, 19.18it/s] 51%|█████     | 107/210 [00:06<00:05, 18.32it/s] 52%|█████▏    | 109/210 [00:06<00:05, 18.09it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.65it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.86it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.75it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.74it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.67it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.32it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.67it/s] 60%|██████    | 126/210 [00:07<00:04, 17.23it/s] 61%|██████    | 128/210 [00:08<00:04, 16.73it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.27it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.49it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.43it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.99it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.90it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.20it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.09it/s] 69%|██████▊   | 144/210 [00:09<00:04, 13.94it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.77it/s] 70%|███████   | 148/210 [00:09<00:04, 15.08it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.38it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.10it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.05it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.87it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.83it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.95it/s] 78%|███████▊  | 163/210 [00:10<00:04, 10.40it/s] 79%|███████▊  | 165/210 [00:10<00:03, 11.87it/s] 80%|███████▉  | 167/210 [00:10<00:03, 13.12it/s] 80%|████████  | 169/210 [00:11<00:03, 13.51it/s] 81%|████████▏ | 171/210 [00:11<00:02, 14.79it/s] 83%|████████▎ | 174/210 [00:11<00:02, 15.71it/s] 84%|████████▍ | 176/210 [00:11<00:02, 16.21it/s] 85%|████████▍ | 178/210 [00:11<00:02, 15.72it/s] 86%|████████▌ | 180/210 [00:11<00:01, 15.88it/s] 87%|████████▋ | 183/210 [00:11<00:01, 17.45it/s] 88%|████████▊ | 185/210 [00:12<00:01, 15.47it/s] 89%|████████▉ | 187/210 [00:12<00:01, 15.31it/s] 90%|█████████ | 190/210 [00:12<00:01, 17.85it/s] 91%|█████████▏| 192/210 [00:12<00:01, 16.97it/s] 92%|█████████▏| 194/210 [00:12<00:00, 16.45it/s] 93%|█████████▎| 196/210 [00:12<00:00, 16.44it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.67it/s] 96%|█████████▌| 202/210 [00:13<00:00, 17.41it/s] 97%|█████████▋| 204/210 [00:13<00:00, 17.50it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.51it/s] 99%|█████████▉| 208/210 [00:13<00:00, 15.20it/s]100%|██████████| 210/210 [00:13<00:00, 15.44it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1863   49]
 [  98   83]]
tn=1863, fp=49, fn=98, tp=83
Number of predicted 0s: 1961
Number of actual 0s: 1912
Number of predicted 1s: 132
Number of actual 1s: 181
Precision: 0.6287878787878788
Accuracy: 0.9297658862876255
Recall: 0.4585635359116022
F1 score: 0.5303514376996805
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=7-step=1648.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:46,  1.96it/s]  1%|▏         | 3/210 [00:00<00:44,  4.67it/s]  2%|▏         | 5/210 [00:00<00:27,  7.46it/s]  3%|▎         | 7/210 [00:00<00:20,  9.81it/s]  4%|▍         | 9/210 [00:01<00:16, 11.87it/s]  5%|▌         | 11/210 [00:01<00:15, 12.58it/s]  6%|▌         | 13/210 [00:01<00:15, 12.77it/s]  7%|▋         | 15/210 [00:01<00:16, 11.96it/s]  8%|▊         | 17/210 [00:01<00:15, 12.83it/s]  9%|▉         | 19/210 [00:01<00:13, 13.70it/s] 10%|█         | 21/210 [00:01<00:12, 14.67it/s] 11%|█         | 23/210 [00:02<00:11, 15.63it/s] 12%|█▏        | 26/210 [00:02<00:11, 16.63it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.31it/s] 14%|█▍        | 30/210 [00:02<00:11, 16.36it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.72it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.46it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.41it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.34it/s] 20%|█▉        | 41/210 [00:03<00:10, 16.19it/s] 20%|██        | 43/210 [00:03<00:10, 15.72it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.68it/s] 23%|██▎       | 48/210 [00:03<00:08, 18.01it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.37it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.65it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.47it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.11it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.34it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.36it/s] 30%|███       | 64/210 [00:04<00:08, 16.86it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.51it/s] 32%|███▏      | 68/210 [00:04<00:09, 15.61it/s] 33%|███▎      | 70/210 [00:04<00:08, 15.67it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.35it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.95it/s] 36%|███▌      | 76/210 [00:05<00:08, 16.03it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.32it/s] 39%|███▊      | 81/210 [00:05<00:08, 15.93it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.77it/s] 40%|████      | 85/210 [00:05<00:07, 17.28it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.35it/s] 42%|████▏     | 89/210 [00:05<00:07, 16.99it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.59it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.21it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.21it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.42it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.16it/s] 50%|█████     | 105/210 [00:06<00:05, 19.33it/s] 51%|█████     | 107/210 [00:06<00:05, 18.39it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.05it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.37it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.65it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.66it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.43it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.43it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.15it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.56it/s] 60%|██████    | 126/210 [00:08<00:04, 17.07it/s] 61%|██████    | 128/210 [00:08<00:04, 16.73it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.36it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.59it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.44it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.83it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.88it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.51it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.17it/s] 69%|██████▊   | 144/210 [00:09<00:04, 13.95it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.59it/s] 70%|███████   | 148/210 [00:09<00:04, 14.89it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.25it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.00it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.99it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.86it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.82it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.93it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.87it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.52it/s] 80%|███████▉  | 167/210 [00:10<00:02, 15.96it/s] 80%|████████  | 169/210 [00:10<00:02, 15.47it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.23it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.56it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.01it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.43it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.75it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.63it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.87it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.42it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.94it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.25it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.54it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.95it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.28it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.09it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.19it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.17it/s] 99%|█████████▉| 208/210 [00:13<00:00, 15.14it/s]100%|██████████| 210/210 [00:13<00:00, 15.64it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1856   56]
 [  92   89]]
tn=1856, fp=56, fn=92, tp=89
Number of predicted 0s: 1948
Number of actual 0s: 1912
Number of predicted 1s: 145
Number of actual 1s: 181
Precision: 0.6137931034482759
Accuracy: 0.9292881032011466
Recall: 0.49171270718232046
F1 score: 0.5460122699386503
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=8-step=1854.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:37,  2.14it/s]  1%|▏         | 3/210 [00:00<00:38,  5.32it/s]  2%|▏         | 5/210 [00:00<00:24,  8.22it/s]  3%|▎         | 7/210 [00:00<00:19, 10.54it/s]  4%|▍         | 9/210 [00:00<00:15, 12.59it/s]  5%|▌         | 11/210 [00:01<00:15, 13.16it/s]  6%|▌         | 13/210 [00:01<00:15, 13.13it/s]  7%|▋         | 15/210 [00:01<00:16, 11.98it/s]  8%|▊         | 17/210 [00:01<00:14, 12.95it/s]  9%|▉         | 19/210 [00:01<00:13, 13.81it/s] 10%|█         | 21/210 [00:01<00:12, 14.73it/s] 11%|█         | 23/210 [00:01<00:11, 15.66it/s] 12%|█▏        | 26/210 [00:02<00:11, 16.54it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.19it/s] 14%|█▍        | 30/210 [00:02<00:11, 16.27it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.55it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.36it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.37it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.32it/s] 20%|█▉        | 41/210 [00:03<00:10, 16.13it/s] 20%|██        | 43/210 [00:03<00:10, 15.69it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.61it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.94it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.32it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.56it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.42it/s] 28%|██▊       | 58/210 [00:03<00:08, 16.97it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.35it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.35it/s] 30%|███       | 64/210 [00:04<00:08, 16.74it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.36it/s] 32%|███▏      | 68/210 [00:04<00:09, 15.62it/s] 33%|███▎      | 70/210 [00:04<00:08, 15.76it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.49it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.07it/s] 36%|███▌      | 76/210 [00:05<00:08, 16.13it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.39it/s] 39%|███▊      | 81/210 [00:05<00:08, 15.93it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.75it/s] 40%|████      | 85/210 [00:05<00:07, 17.32it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.42it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.06it/s] 43%|████▎     | 91/210 [00:05<00:06, 17.66it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.29it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.25it/s] 47%|████▋     | 98/210 [00:06<00:05, 19.41it/s] 48%|████▊     | 100/210 [00:06<00:05, 18.86it/s] 49%|████▉     | 103/210 [00:06<00:05, 19.75it/s] 50%|█████     | 105/210 [00:06<00:05, 18.83it/s] 51%|█████     | 107/210 [00:06<00:05, 18.05it/s] 52%|█████▏    | 109/210 [00:06<00:05, 17.81it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.17it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.69it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.72it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.48it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.50it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.12it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.52it/s] 60%|██████    | 126/210 [00:07<00:04, 17.02it/s] 61%|██████    | 128/210 [00:08<00:04, 16.70it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.15it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.48it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.38it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.85it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.87it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.39it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.15it/s] 69%|██████▊   | 144/210 [00:09<00:04, 13.91it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.61it/s] 70%|███████   | 148/210 [00:09<00:04, 14.96it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.31it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.03it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.02it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.86it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.82it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.94it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.93it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.66it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.04it/s] 80%|████████  | 169/210 [00:10<00:02, 15.40it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.15it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.53it/s] 84%|████████▍ | 177/210 [00:11<00:01, 16.91it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.41it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.81it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.78it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.85it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.40it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.88it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.13it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.52it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.98it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.26it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.06it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.14it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.05it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.97it/s]100%|██████████| 210/210 [00:13<00:00, 15.72it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1869   43]
 [  97   84]]
tn=1869, fp=43, fn=97, tp=84
Number of predicted 0s: 1966
Number of actual 0s: 1912
Number of predicted 1s: 127
Number of actual 1s: 181
Precision: 0.6614173228346457
Accuracy: 0.9331103678929766
Recall: 0.46408839779005523
F1 score: 0.5454545454545454
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69931/checkpoints/converted/epoch=9-step=2060.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:35,  2.20it/s]  1%|▏         | 3/210 [00:00<00:40,  5.05it/s]  2%|▏         | 5/210 [00:00<00:25,  7.93it/s]  3%|▎         | 7/210 [00:00<00:19, 10.27it/s]  4%|▍         | 9/210 [00:01<00:16, 12.32it/s]  5%|▌         | 11/210 [00:01<00:15, 12.89it/s]  6%|▌         | 13/210 [00:01<00:15, 13.01it/s]  7%|▋         | 15/210 [00:01<00:16, 12.09it/s]  8%|▊         | 17/210 [00:01<00:14, 12.99it/s]  9%|▉         | 19/210 [00:01<00:13, 13.78it/s] 10%|█         | 21/210 [00:01<00:12, 14.65it/s] 11%|█         | 23/210 [00:01<00:12, 15.58it/s] 12%|█▏        | 26/210 [00:02<00:11, 16.59it/s] 13%|█▎        | 28/210 [00:02<00:10, 17.29it/s] 14%|█▍        | 30/210 [00:02<00:10, 16.37it/s] 15%|█▌        | 32/210 [00:02<00:10, 16.75it/s] 17%|█▋        | 35/210 [00:02<00:09, 18.43it/s] 18%|█▊        | 37/210 [00:02<00:10, 16.43it/s] 19%|█▊        | 39/210 [00:02<00:10, 16.33it/s] 20%|█▉        | 41/210 [00:03<00:10, 16.14it/s] 20%|██        | 43/210 [00:03<00:10, 15.67it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.63it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.97it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.33it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.62it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.37it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.03it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.35it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.25it/s] 30%|███       | 64/210 [00:04<00:08, 16.74it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.38it/s] 32%|███▏      | 68/210 [00:04<00:09, 15.61it/s] 33%|███▎      | 70/210 [00:04<00:08, 15.78it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.49it/s] 35%|███▌      | 74/210 [00:04<00:07, 17.03it/s] 36%|███▌      | 76/210 [00:05<00:08, 16.13it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.39it/s] 39%|███▊      | 81/210 [00:05<00:08, 15.97it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.75it/s] 40%|████      | 85/210 [00:05<00:07, 17.29it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.42it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.07it/s] 43%|████▎     | 91/210 [00:05<00:06, 17.66it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.29it/s] 46%|████▌     | 96/210 [00:06<00:05, 19.22it/s] 47%|████▋     | 98/210 [00:06<00:05, 19.37it/s] 48%|████▊     | 100/210 [00:06<00:05, 18.83it/s] 49%|████▉     | 103/210 [00:06<00:05, 19.69it/s] 50%|█████     | 105/210 [00:06<00:05, 18.87it/s] 51%|█████     | 107/210 [00:06<00:05, 18.08it/s] 52%|█████▏    | 109/210 [00:06<00:05, 17.85it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.24it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.64it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.68it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.53it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.51it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.14it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.56it/s] 60%|██████    | 126/210 [00:07<00:04, 17.08it/s] 61%|██████    | 128/210 [00:08<00:04, 16.72it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.22it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.47it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.38it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.88it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.87it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.32it/s] 68%|██████▊   | 142/210 [00:08<00:04, 15.13it/s] 69%|██████▊   | 144/210 [00:09<00:04, 13.91it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.66it/s] 70%|███████   | 148/210 [00:09<00:04, 14.98it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.33it/s] 72%|███████▏  | 152/210 [00:09<00:04, 12.04it/s] 74%|███████▍  | 155/210 [00:09<00:03, 14.04it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.88it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.86it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.95it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.97it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.66it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.02it/s] 80%|████████  | 169/210 [00:10<00:02, 15.44it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.20it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.58it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.00it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.45it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.83it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.76it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.82it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.41it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.93it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.21it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.58it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.93it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.38it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.24it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.22it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.25it/s] 99%|█████████▉| 208/210 [00:13<00:00, 15.10it/s]100%|██████████| 210/210 [00:13<00:00, 15.73it/s]
[[1860   52]
 [  89   92]]
tn=1860, fp=52, fn=89, tp=92
Number of predicted 0s: 1949
Number of actual 0s: 1912
Number of predicted 1s: 144
Number of actual 1s: 181
Precision: 0.6388888888888888
Accuracy: 0.9326325848064978
Recall: 0.5082872928176796
F1 score: 0.5661538461538461
{'epoch=0-step=206.ckpt': {'accuracy': 0.9034878165312948, 'precision': 0.4620938628158845, 'recall': 0.7071823204419889, 'f1_score': 0.5589519650655023}, 'epoch=1-step=412.ckpt': {'accuracy': 0.884854276158624, 'precision': 0.4127906976744186, 'recall': 0.7845303867403315, 'f1_score': 0.5409523809523809}, 'epoch=2-step=618.ckpt': {'accuracy': 0.9288103201146679, 'precision': 0.5869565217391305, 'recall': 0.5966850828729282, 'f1_score': 0.5917808219178081}, 'epoch=3-step=824.ckpt': {'accuracy': 0.9283325370281892, 'precision': 0.6260162601626016, 'recall': 0.425414364640884, 'f1_score': 0.5065789473684209}, 'epoch=4-step=1030.ckpt': {'accuracy': 0.9273769708552317, 'precision': 0.6260869565217392, 'recall': 0.39779005524861877, 'f1_score': 0.4864864864864865}, 'epoch=5-step=1236.ckpt': {'accuracy': 0.9297658862876255, 'precision': 0.5977011494252874, 'recall': 0.574585635359116, 'f1_score': 0.5859154929577465}, 'epoch=6-step=1442.ckpt': {'accuracy': 0.9297658862876255, 'precision': 0.6287878787878788, 'recall': 0.4585635359116022, 'f1_score': 0.5303514376996805}, 'epoch=7-step=1648.ckpt': {'accuracy': 0.9292881032011466, 'precision': 0.6137931034482759, 'recall': 0.49171270718232046, 'f1_score': 0.5460122699386503}, 'epoch=8-step=1854.ckpt': {'accuracy': 0.9331103678929766, 'precision': 0.6614173228346457, 'recall': 0.46408839779005523, 'f1_score': 0.5454545454545454}, 'epoch=9-step=2060.ckpt': {'accuracy': 0.9326325848064978, 'precision': 0.6388888888888888, 'recall': 0.5082872928176796, 'f1_score': 0.5661538461538461}}
