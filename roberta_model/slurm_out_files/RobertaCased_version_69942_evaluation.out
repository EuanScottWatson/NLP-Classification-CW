Mon Mar  6 21:10:42 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.11    Driver Version: 525.60.11    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:0A.0 Off |                    0 |
| N/A   35C    P8     9W /  70W |      2MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
 21:10:42 up 5 days,  7:24,  1 user,  load average: 1.08, 1.02, 0.83
Folder of Checkpoints: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted
Test File: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Config: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/configs/RoBERTa_config_cased.json
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Testing checkpoints found in /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted
10 checkpoints found
Testing...
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=0-step=206.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]  0%|          | 1/210 [00:06<21:56,  6.30s/it]  1%|▏         | 3/210 [00:06<05:53,  1.71s/it]  2%|▏         | 5/210 [00:06<02:56,  1.16it/s]  3%|▎         | 7/210 [00:06<01:45,  1.93it/s]  4%|▍         | 9/210 [00:06<01:08,  2.92it/s]  5%|▌         | 11/210 [00:06<00:49,  4.06it/s]  6%|▌         | 13/210 [00:07<00:37,  5.24it/s]  7%|▋         | 15/210 [00:07<00:31,  6.16it/s]  8%|▊         | 17/210 [00:07<00:25,  7.63it/s]  9%|▉         | 19/210 [00:07<00:20,  9.25it/s] 10%|█         | 21/210 [00:07<00:17, 10.86it/s] 11%|█         | 23/210 [00:07<00:15, 12.32it/s] 12%|█▏        | 25/210 [00:07<00:13, 13.85it/s] 13%|█▎        | 27/210 [00:08<00:12, 14.40it/s] 14%|█▍        | 29/210 [00:08<00:11, 15.66it/s] 15%|█▍        | 31/210 [00:08<00:11, 15.26it/s] 16%|█▌        | 34/210 [00:08<00:10, 17.49it/s] 17%|█▋        | 36/210 [00:08<00:10, 16.19it/s] 18%|█▊        | 38/210 [00:08<00:10, 16.12it/s] 19%|█▉        | 40/210 [00:08<00:10, 15.86it/s] 20%|██        | 42/210 [00:08<00:11, 15.05it/s] 21%|██▏       | 45/210 [00:09<00:09, 16.68it/s] 23%|██▎       | 48/210 [00:09<00:09, 17.75it/s] 24%|██▍       | 51/210 [00:09<00:08, 18.33it/s] 26%|██▌       | 54/210 [00:09<00:08, 18.98it/s] 27%|██▋       | 56/210 [00:09<00:08, 17.92it/s] 28%|██▊       | 58/210 [00:09<00:08, 17.63it/s] 29%|██▊       | 60/210 [00:09<00:08, 17.94it/s] 30%|██▉       | 62/210 [00:10<00:09, 16.30it/s] 30%|███       | 64/210 [00:10<00:08, 16.94it/s] 32%|███▏      | 67/210 [00:10<00:08, 16.46it/s] 33%|███▎      | 69/210 [00:10<00:09, 15.36it/s] 34%|███▍      | 71/210 [00:10<00:08, 15.98it/s] 35%|███▌      | 74/210 [00:10<00:08, 16.60it/s] 36%|███▌      | 76/210 [00:10<00:08, 15.85it/s] 37%|███▋      | 78/210 [00:11<00:09, 14.64it/s] 39%|███▊      | 81/210 [00:11<00:07, 16.27it/s] 40%|████      | 84/210 [00:11<00:07, 17.07it/s] 41%|████▏     | 87/210 [00:11<00:06, 17.87it/s] 42%|████▏     | 89/210 [00:11<00:06, 17.48it/s] 43%|████▎     | 91/210 [00:11<00:06, 17.73it/s] 44%|████▍     | 93/210 [00:11<00:06, 17.47it/s] 46%|████▌     | 96/210 [00:12<00:05, 19.05it/s] 47%|████▋     | 99/210 [00:12<00:05, 18.78it/s] 49%|████▊     | 102/210 [00:12<00:05, 20.50it/s] 50%|█████     | 105/210 [00:12<00:05, 19.52it/s] 51%|█████     | 107/210 [00:12<00:05, 18.63it/s] 52%|█████▏    | 109/210 [00:12<00:05, 18.46it/s] 53%|█████▎    | 111/210 [00:12<00:05, 18.00it/s] 54%|█████▍    | 114/210 [00:13<00:05, 18.07it/s] 55%|█████▌    | 116/210 [00:13<00:05, 17.72it/s] 56%|█████▌    | 118/210 [00:13<00:05, 17.87it/s] 57%|█████▋    | 120/210 [00:13<00:05, 16.88it/s] 58%|█████▊    | 122/210 [00:13<00:05, 17.57it/s] 59%|█████▉    | 124/210 [00:13<00:04, 17.28it/s] 60%|██████    | 126/210 [00:13<00:04, 17.67it/s] 61%|██████    | 128/210 [00:13<00:04, 17.33it/s] 62%|██████▏   | 130/210 [00:13<00:04, 16.31it/s] 63%|██████▎   | 132/210 [00:14<00:04, 16.67it/s] 64%|██████▍   | 134/210 [00:14<00:04, 16.15it/s] 65%|██████▍   | 136/210 [00:14<00:04, 16.77it/s] 66%|██████▌   | 138/210 [00:14<00:04, 16.05it/s] 67%|██████▋   | 140/210 [00:14<00:04, 17.01it/s] 68%|██████▊   | 142/210 [00:14<00:04, 15.73it/s] 69%|██████▊   | 144/210 [00:14<00:04, 14.46it/s] 70%|██████▉   | 146/210 [00:14<00:04, 15.29it/s] 70%|███████   | 148/210 [00:15<00:04, 15.43it/s] 71%|███████▏  | 150/210 [00:15<00:05, 11.33it/s] 72%|███████▏  | 152/210 [00:15<00:04, 11.78it/s] 74%|███████▍  | 155/210 [00:15<00:03, 14.00it/s] 75%|███████▍  | 157/210 [00:15<00:03, 14.55it/s] 76%|███████▌  | 159/210 [00:15<00:03, 15.52it/s] 77%|███████▋  | 161/210 [00:16<00:03, 15.81it/s] 78%|███████▊  | 163/210 [00:16<00:03, 15.02it/s] 79%|███████▊  | 165/210 [00:16<00:02, 15.94it/s] 80%|███████▉  | 167/210 [00:16<00:02, 16.45it/s] 80%|████████  | 169/210 [00:16<00:02, 15.91it/s] 81%|████████▏ | 171/210 [00:16<00:02, 16.65it/s] 83%|████████▎ | 174/210 [00:16<00:02, 16.78it/s] 84%|████████▍ | 177/210 [00:17<00:01, 17.28it/s] 85%|████████▌ | 179/210 [00:17<00:01, 16.60it/s] 86%|████████▌ | 181/210 [00:17<00:01, 16.89it/s] 88%|████████▊ | 184/210 [00:17<00:01, 16.81it/s] 89%|████████▊ | 186/210 [00:17<00:01, 16.98it/s] 90%|████████▉ | 188/210 [00:17<00:01, 16.64it/s] 91%|█████████ | 191/210 [00:17<00:01, 18.15it/s] 92%|█████████▏| 193/210 [00:17<00:00, 17.25it/s] 93%|█████████▎| 195/210 [00:18<00:00, 16.78it/s] 94%|█████████▍| 197/210 [00:18<00:00, 17.36it/s] 95%|█████████▍| 199/210 [00:18<00:00, 15.85it/s] 96%|█████████▌| 202/210 [00:18<00:00, 17.58it/s] 97%|█████████▋| 204/210 [00:18<00:00, 17.47it/s] 98%|█████████▊| 206/210 [00:18<00:00, 17.38it/s] 99%|█████████▉| 208/210 [00:18<00:00, 15.02it/s]100%|██████████| 210/210 [00:19<00:00, 11.04it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[ 222 1690]
 [   3  178]]
tn=222, fp=1690, fn=3, tp=178
Number of predicted 0s: 225
Number of actual 0s: 1912
Number of predicted 1s: 1868
Number of actual 1s: 181
Precision: 0.09528907922912205
Accuracy: 0.19111323459149546
Recall: 0.9834254143646409
F1 score: 0.17374328940946804
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=1-step=412.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:46,  1.96it/s]  1%|▏         | 3/210 [00:00<00:41,  4.93it/s]  2%|▏         | 5/210 [00:00<00:26,  7.81it/s]  3%|▎         | 7/210 [00:00<00:19, 10.15it/s]  4%|▍         | 9/210 [00:01<00:16, 12.33it/s]  5%|▌         | 11/210 [00:01<00:15, 13.01it/s]  6%|▌         | 13/210 [00:01<00:15, 12.78it/s]  7%|▋         | 15/210 [00:01<00:16, 11.56it/s]  8%|▊         | 17/210 [00:01<00:15, 12.67it/s]  9%|▉         | 19/210 [00:01<00:13, 13.83it/s] 10%|█         | 21/210 [00:01<00:12, 14.84it/s] 11%|█         | 23/210 [00:02<00:11, 15.66it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.58it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.24it/s] 14%|█▍        | 29/210 [00:02<00:10, 17.07it/s] 15%|█▍        | 31/210 [00:02<00:11, 16.06it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.95it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.63it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.47it/s] 19%|█▉        | 40/210 [00:03<00:10, 16.05it/s] 20%|██        | 42/210 [00:03<00:11, 15.07it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.59it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.58it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.13it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.79it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.75it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.36it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.72it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.20it/s] 30%|███       | 64/210 [00:04<00:08, 16.87it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.63it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.94it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.25it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.04it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.35it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.70it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.37it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.17it/s] 40%|███▉      | 83/210 [00:05<00:07, 17.02it/s] 40%|████      | 85/210 [00:05<00:07, 17.42it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.51it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.00it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.70it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.26it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.97it/s] 47%|████▋     | 99/210 [00:06<00:05, 18.64it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.22it/s] 50%|█████     | 105/210 [00:06<00:05, 19.23it/s] 51%|█████     | 107/210 [00:06<00:05, 18.51it/s] 52%|█████▏    | 109/210 [00:06<00:05, 18.35it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.78it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.84it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.52it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.71it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.70it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.14it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.83it/s] 60%|██████    | 126/210 [00:07<00:04, 17.22it/s] 61%|██████    | 128/210 [00:08<00:04, 17.00it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.08it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.53it/s] 64%|██████▍   | 134/210 [00:08<00:04, 16.01it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.62it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.92it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.89it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.64it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.32it/s] 70%|██████▉   | 146/210 [00:09<00:04, 15.16it/s] 70%|███████   | 148/210 [00:09<00:04, 15.35it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.30it/s] 72%|███████▏  | 152/210 [00:09<00:04, 11.75it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.93it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.49it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.49it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.72it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.83it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.86it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.34it/s] 80%|████████  | 169/210 [00:10<00:02, 15.86it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.55it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.75it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.23it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.55it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.80it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.86it/s] 89%|████████▊ | 186/210 [00:11<00:01, 17.11it/s] 90%|████████▉ | 188/210 [00:11<00:01, 16.65it/s] 91%|█████████ | 191/210 [00:12<00:01, 18.15it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.04it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.65it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.13it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.73it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.38it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.26it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.15it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.87it/s]100%|██████████| 210/210 [00:13<00:00, 15.70it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1756  156]
 [ 130   51]]
tn=1756, fp=156, fn=130, tp=51
Number of predicted 0s: 1886
Number of actual 0s: 1912
Number of predicted 1s: 207
Number of actual 1s: 181
Precision: 0.2463768115942029
Accuracy: 0.8633540372670807
Recall: 0.281767955801105
F1 score: 0.26288659793814434
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=2-step=618.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:40,  2.08it/s]  1%|▏         | 3/210 [00:00<00:43,  4.77it/s]  2%|▏         | 5/210 [00:00<00:26,  7.65it/s]  3%|▎         | 7/210 [00:00<00:20,  9.99it/s]  4%|▍         | 9/210 [00:01<00:16, 12.21it/s]  5%|▌         | 11/210 [00:01<00:15, 12.94it/s]  6%|▌         | 13/210 [00:01<00:15, 12.72it/s]  7%|▋         | 15/210 [00:01<00:16, 11.55it/s]  8%|▊         | 17/210 [00:01<00:15, 12.56it/s]  9%|▉         | 19/210 [00:01<00:13, 13.70it/s] 10%|█         | 21/210 [00:01<00:12, 14.69it/s] 11%|█         | 23/210 [00:02<00:12, 15.39it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.30it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.02it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.98it/s] 15%|█▍        | 31/210 [00:02<00:11, 16.03it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.91it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.54it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.43it/s] 19%|█▉        | 40/210 [00:03<00:10, 16.11it/s] 20%|██        | 42/210 [00:03<00:11, 15.06it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.59it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.71it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.26it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.82it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.66it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.40it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.74it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.10it/s] 30%|███       | 64/210 [00:04<00:08, 16.71it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.51it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.94it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.18it/s] 34%|███▍      | 72/210 [00:04<00:08, 15.96it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.33it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.70it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.34it/s] 39%|███▊      | 81/210 [00:05<00:07, 16.20it/s] 40%|███▉      | 83/210 [00:05<00:07, 17.00it/s] 40%|████      | 85/210 [00:05<00:07, 17.44it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.55it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.03it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.63it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.34it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.96it/s] 47%|████▋     | 99/210 [00:06<00:05, 18.58it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.34it/s] 50%|█████     | 105/210 [00:06<00:05, 19.23it/s] 51%|█████     | 107/210 [00:06<00:05, 18.48it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.25it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.72it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.75it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.39it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.60it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.56it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.27it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.88it/s] 60%|██████    | 126/210 [00:08<00:04, 17.20it/s] 61%|██████    | 128/210 [00:08<00:04, 17.02it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.14it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.51it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.95it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.51it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.83it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.82it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.53it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.32it/s] 70%|██████▉   | 146/210 [00:09<00:04, 15.15it/s] 70%|███████   | 148/210 [00:09<00:04, 15.31it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.27it/s] 72%|███████▏  | 152/210 [00:09<00:04, 11.74it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.91it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.44it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.41it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.66it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.83it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.70it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.18it/s] 80%|████████  | 169/210 [00:10<00:02, 15.63it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.40it/s] 82%|████████▏ | 173/210 [00:11<00:02, 17.29it/s] 83%|████████▎ | 175/210 [00:11<00:02, 16.63it/s] 84%|████████▍ | 177/210 [00:11<00:01, 16.94it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.21it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.65it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.62it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.70it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.25it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.74it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.91it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.60it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.13it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.75it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.40it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.23it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.14it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.84it/s]100%|██████████| 210/210 [00:13<00:00, 15.60it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1502  410]
 [  77  104]]
tn=1502, fp=410, fn=77, tp=104
Number of predicted 0s: 1579
Number of actual 0s: 1912
Number of predicted 1s: 514
Number of actual 1s: 181
Precision: 0.20233463035019456
Accuracy: 0.7673196368848543
Recall: 0.574585635359116
F1 score: 0.2992805755395683
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=3-step=824.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:40,  2.07it/s]  1%|▏         | 3/210 [00:00<00:40,  5.15it/s]  2%|▏         | 5/210 [00:00<00:25,  7.97it/s]  3%|▎         | 7/210 [00:00<00:19, 10.35it/s]  4%|▍         | 9/210 [00:01<00:16, 12.47it/s]  5%|▌         | 11/210 [00:01<00:15, 13.06it/s]  6%|▌         | 13/210 [00:01<00:15, 12.76it/s]  7%|▋         | 15/210 [00:01<00:16, 11.57it/s]  8%|▊         | 17/210 [00:01<00:15, 12.63it/s]  9%|▉         | 19/210 [00:01<00:13, 13.71it/s] 10%|█         | 21/210 [00:01<00:12, 14.71it/s] 11%|█         | 23/210 [00:01<00:12, 15.41it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.38it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.10it/s] 14%|█▍        | 29/210 [00:02<00:10, 17.01it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.99it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.84it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.39it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.32it/s] 19%|█▉        | 40/210 [00:03<00:10, 15.89it/s] 20%|██        | 42/210 [00:03<00:11, 14.95it/s] 21%|██▏       | 45/210 [00:03<00:10, 16.47it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.52it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.09it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.70it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.64it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.33it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.68it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.16it/s] 30%|███       | 64/210 [00:04<00:08, 16.84it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.58it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.91it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.27it/s] 34%|███▍      | 72/210 [00:04<00:08, 16.04it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.38it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.57it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.23it/s] 39%|███▊      | 81/210 [00:05<00:08, 16.06it/s] 40%|████      | 84/210 [00:05<00:07, 16.87it/s] 41%|████▏     | 87/210 [00:05<00:06, 17.65it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.18it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.80it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.44it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.96it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.45it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.05it/s] 50%|█████     | 105/210 [00:06<00:05, 19.13it/s] 51%|█████     | 107/210 [00:06<00:05, 18.31it/s] 52%|█████▏    | 109/210 [00:06<00:05, 18.20it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.76it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.91it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.52it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.54it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.62it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.17it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.89it/s] 60%|██████    | 126/210 [00:07<00:04, 17.35it/s] 61%|██████    | 128/210 [00:08<00:04, 16.93it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.05it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.43it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.89it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.53it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.80it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.75it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.60it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.26it/s] 70%|██████▉   | 146/210 [00:09<00:04, 15.11it/s] 70%|███████   | 148/210 [00:09<00:04, 15.30it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.18it/s] 72%|███████▏  | 152/210 [00:09<00:04, 11.63it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.81it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.36it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.36it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.69it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.81it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.66it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.13it/s] 80%|████████  | 169/210 [00:10<00:02, 15.52it/s] 81%|████████▏ | 171/210 [00:10<00:02, 16.24it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.51it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.10it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.44it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.70it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.63it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.80it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.37it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.94it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.98it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.62it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.14it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.69it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.38it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.23it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.16it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.88it/s]100%|██████████| 210/210 [00:13<00:00, 15.65it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1606  306]
 [  99   82]]
tn=1606, fp=306, fn=99, tp=82
Number of predicted 0s: 1705
Number of actual 0s: 1912
Number of predicted 1s: 388
Number of actual 1s: 181
Precision: 0.211340206185567
Accuracy: 0.8064978499761108
Recall: 0.4530386740331492
F1 score: 0.28822495606326887
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=4-step=1030.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:40,  2.07it/s]  1%|▏         | 3/210 [00:00<00:40,  5.09it/s]  2%|▏         | 5/210 [00:00<00:25,  8.01it/s]  3%|▎         | 7/210 [00:00<00:19, 10.34it/s]  4%|▍         | 9/210 [00:01<00:16, 12.50it/s]  5%|▌         | 11/210 [00:01<00:15, 13.10it/s]  6%|▌         | 13/210 [00:01<00:15, 12.86it/s]  7%|▋         | 15/210 [00:01<00:16, 11.48it/s]  8%|▊         | 17/210 [00:01<00:15, 12.57it/s]  9%|▉         | 19/210 [00:01<00:13, 13.78it/s] 10%|█         | 21/210 [00:01<00:12, 14.76it/s] 11%|█         | 23/210 [00:01<00:12, 15.57it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.49it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.13it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.92it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.84it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.76it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.40it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.24it/s] 19%|█▉        | 40/210 [00:03<00:10, 15.89it/s] 20%|██        | 42/210 [00:03<00:11, 14.94it/s] 21%|██▏       | 45/210 [00:03<00:09, 16.52it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.55it/s] 24%|██▍       | 51/210 [00:03<00:08, 18.07it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.55it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.49it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.23it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.56it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.08it/s] 30%|███       | 64/210 [00:04<00:08, 16.73it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.52it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.89it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.21it/s] 34%|███▍      | 72/210 [00:04<00:08, 15.98it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.40it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.58it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.26it/s] 39%|███▊      | 81/210 [00:05<00:08, 16.08it/s] 40%|████      | 84/210 [00:05<00:07, 16.89it/s] 41%|████▏     | 87/210 [00:05<00:06, 17.64it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.19it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.76it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.38it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.92it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.45it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.10it/s] 50%|█████     | 105/210 [00:06<00:05, 19.15it/s] 51%|█████     | 107/210 [00:06<00:05, 18.34it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.23it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.80it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.90it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.53it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.54it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.64it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.21it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.89it/s] 60%|██████    | 126/210 [00:07<00:04, 17.37it/s] 61%|██████    | 128/210 [00:08<00:04, 16.96it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.03it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.39it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.86it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.53it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.78it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.78it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.58it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.23it/s] 70%|██████▉   | 146/210 [00:09<00:04, 15.01it/s] 70%|███████   | 148/210 [00:09<00:04, 15.23it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.21it/s] 72%|███████▏  | 152/210 [00:09<00:04, 11.63it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.82it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.41it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.39it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.65it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.80it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.67it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.12it/s] 80%|████████  | 169/210 [00:10<00:02, 15.57it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.30it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.55it/s] 84%|████████▍ | 177/210 [00:11<00:01, 16.96it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.39it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.71it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.63it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.84it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.42it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.89it/s] 92%|█████████▏| 193/210 [00:12<00:00, 17.00it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.56it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.09it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.55it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.25it/s] 97%|█████████▋| 204/210 [00:12<00:00, 17.21it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.23it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.88it/s]100%|██████████| 210/210 [00:13<00:00, 15.63it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1830   82]
 [ 144   37]]
tn=1830, fp=82, fn=144, tp=37
Number of predicted 0s: 1974
Number of actual 0s: 1912
Number of predicted 1s: 119
Number of actual 1s: 181
Precision: 0.31092436974789917
Accuracy: 0.8920210224558051
Recall: 0.20441988950276244
F1 score: 0.24666666666666665
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=5-step=1236.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:41,  2.07it/s]  1%|▏         | 3/210 [00:00<00:41,  5.02it/s]  2%|▏         | 5/210 [00:00<00:25,  7.89it/s]  3%|▎         | 7/210 [00:00<00:19, 10.18it/s]  4%|▍         | 9/210 [00:01<00:16, 12.32it/s]  5%|▌         | 11/210 [00:01<00:15, 12.92it/s]  6%|▌         | 13/210 [00:01<00:15, 12.63it/s]  7%|▋         | 15/210 [00:01<00:17, 11.34it/s]  8%|▊         | 17/210 [00:01<00:15, 12.40it/s]  9%|▉         | 19/210 [00:01<00:14, 13.58it/s] 10%|█         | 21/210 [00:01<00:12, 14.63it/s] 11%|█         | 23/210 [00:02<00:12, 15.38it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.36it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.06it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.84it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.75it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.69it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.34it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.23it/s] 19%|█▉        | 40/210 [00:03<00:10, 15.91it/s] 20%|██        | 42/210 [00:03<00:11, 14.94it/s] 21%|██▏       | 45/210 [00:03<00:10, 16.50it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.40it/s] 24%|██▍       | 51/210 [00:03<00:08, 17.97it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.61it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.55it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.32it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.63it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.08it/s] 30%|███       | 64/210 [00:04<00:08, 16.78it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.48it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.87it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.12it/s] 34%|███▍      | 72/210 [00:04<00:08, 15.92it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.38it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.58it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.18it/s] 39%|███▊      | 81/210 [00:05<00:08, 16.02it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.81it/s] 40%|████      | 85/210 [00:05<00:07, 17.25it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.27it/s] 42%|████▏     | 89/210 [00:05<00:07, 16.80it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.56it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.11it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.76it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.37it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.09it/s] 50%|█████     | 105/210 [00:06<00:05, 19.06it/s] 51%|█████     | 107/210 [00:06<00:05, 18.27it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.10it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.51it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.60it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.32it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.48it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.49it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.14it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.75it/s] 60%|██████    | 126/210 [00:08<00:04, 17.15it/s] 61%|██████    | 128/210 [00:08<00:04, 16.97it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.02it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.48it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.99it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.51it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.82it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.74it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.45it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.07it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.89it/s] 70%|███████   | 148/210 [00:09<00:04, 15.14it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.15it/s] 72%|███████▏  | 152/210 [00:09<00:04, 11.63it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.77it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.24it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.16it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.50it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.68it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.69it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.18it/s] 80%|████████  | 169/210 [00:10<00:02, 15.69it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.42it/s] 82%|████████▏ | 173/210 [00:11<00:02, 17.33it/s] 83%|████████▎ | 175/210 [00:11<00:02, 16.65it/s] 84%|████████▍ | 177/210 [00:11<00:01, 16.78it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.24it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.59it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.45it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.68it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.31it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.85it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.80it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.42it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.92it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.57it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.27it/s] 97%|█████████▋| 204/210 [00:13<00:00, 17.18it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.00it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.79it/s]100%|██████████| 210/210 [00:13<00:00, 15.54it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1822   90]
 [ 139   42]]
tn=1822, fp=90, fn=139, tp=42
Number of predicted 0s: 1961
Number of actual 0s: 1912
Number of predicted 1s: 132
Number of actual 1s: 181
Precision: 0.3181818181818182
Accuracy: 0.8905876731963689
Recall: 0.23204419889502761
F1 score: 0.268370607028754
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=6-step=1442.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:48,  1.92it/s]  1%|▏         | 3/210 [00:00<00:45,  4.57it/s]  2%|▏         | 5/210 [00:00<00:27,  7.36it/s]  3%|▎         | 7/210 [00:00<00:20,  9.67it/s]  4%|▍         | 9/210 [00:01<00:16, 11.85it/s]  5%|▌         | 11/210 [00:01<00:15, 12.60it/s]  6%|▌         | 13/210 [00:01<00:15, 12.42it/s]  7%|▋         | 15/210 [00:01<00:17, 11.26it/s]  8%|▊         | 17/210 [00:01<00:15, 12.24it/s]  9%|▉         | 19/210 [00:01<00:14, 13.47it/s] 10%|█         | 21/210 [00:01<00:13, 14.46it/s] 11%|█         | 23/210 [00:02<00:12, 15.17it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.22it/s] 13%|█▎        | 27/210 [00:02<00:11, 15.92it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.77it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.81it/s] 16%|█▌        | 34/210 [00:02<00:10, 17.46it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.24it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.09it/s] 19%|█▉        | 40/210 [00:03<00:10, 15.72it/s] 20%|██        | 42/210 [00:03<00:11, 14.86it/s] 21%|██▏       | 45/210 [00:03<00:10, 16.40it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.45it/s] 24%|██▍       | 51/210 [00:03<00:08, 17.95it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.55it/s] 27%|██▋       | 56/210 [00:04<00:08, 17.46it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.16it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.51it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.04it/s] 30%|███       | 64/210 [00:04<00:08, 16.58it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.28it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.70it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.05it/s] 34%|███▍      | 72/210 [00:05<00:08, 15.76it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.19it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.43it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.12it/s] 39%|███▊      | 81/210 [00:05<00:08, 16.03it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.81it/s] 40%|████      | 85/210 [00:05<00:07, 17.21it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.26it/s] 42%|████▏     | 89/210 [00:06<00:07, 16.73it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.45it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.19it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.87it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.49it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.07it/s] 50%|█████     | 105/210 [00:06<00:05, 19.04it/s] 51%|█████     | 107/210 [00:07<00:05, 18.20it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.01it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.52it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.57it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.30it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.49it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.43it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.06it/s] 59%|█████▉    | 124/210 [00:08<00:05, 16.64it/s] 60%|██████    | 126/210 [00:08<00:04, 17.08it/s] 61%|██████    | 128/210 [00:08<00:04, 16.84it/s] 62%|██████▏   | 130/210 [00:08<00:05, 15.92it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.31it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.78it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.38it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.75it/s] 67%|██████▋   | 140/210 [00:09<00:04, 16.67it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.50it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.15it/s] 70%|██████▉   | 146/210 [00:09<00:04, 15.00it/s] 70%|███████   | 148/210 [00:09<00:04, 15.21it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.11it/s] 72%|███████▏  | 152/210 [00:10<00:05, 11.56it/s] 74%|███████▍  | 155/210 [00:10<00:03, 13.76it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.25it/s] 76%|███████▌  | 159/210 [00:10<00:04, 11.13it/s] 77%|███████▋  | 161/210 [00:10<00:04, 12.23it/s] 78%|███████▊  | 163/210 [00:10<00:03, 12.45it/s] 79%|███████▊  | 165/210 [00:10<00:03, 13.75it/s] 80%|███████▉  | 167/210 [00:11<00:02, 14.65it/s] 80%|████████  | 169/210 [00:11<00:02, 14.64it/s] 81%|████████▏ | 171/210 [00:11<00:02, 15.64it/s] 82%|████████▏ | 173/210 [00:11<00:02, 16.29it/s] 83%|████████▎ | 175/210 [00:11<00:02, 15.90it/s] 84%|████████▍ | 177/210 [00:11<00:02, 16.28it/s] 85%|████████▌ | 179/210 [00:11<00:01, 15.81it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.31it/s] 88%|████████▊ | 184/210 [00:12<00:01, 16.35it/s] 89%|████████▊ | 186/210 [00:12<00:01, 16.64it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.18it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.59it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.70it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.46it/s] 94%|█████████▍| 197/210 [00:12<00:00, 17.04it/s] 95%|█████████▍| 199/210 [00:13<00:00, 15.69it/s] 96%|█████████▌| 202/210 [00:13<00:00, 17.36it/s] 97%|█████████▋| 204/210 [00:13<00:00, 17.20it/s] 98%|█████████▊| 206/210 [00:13<00:00, 17.00it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.70it/s]100%|██████████| 210/210 [00:13<00:00, 15.23it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1848   64]
 [ 147   34]]
tn=1848, fp=64, fn=147, tp=34
Number of predicted 0s: 1995
Number of actual 0s: 1912
Number of predicted 1s: 98
Number of actual 1s: 181
Precision: 0.3469387755102041
Accuracy: 0.8991877687529861
Recall: 0.1878453038674033
F1 score: 0.2437275985663082
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=7-step=1648.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:42,  2.03it/s]  1%|▏         | 3/210 [00:00<00:43,  4.78it/s]  2%|▏         | 5/210 [00:00<00:26,  7.64it/s]  3%|▎         | 7/210 [00:00<00:20,  9.95it/s]  4%|▍         | 9/210 [00:01<00:16, 12.11it/s]  5%|▌         | 11/210 [00:01<00:15, 12.80it/s]  6%|▌         | 13/210 [00:01<00:15, 12.57it/s]  7%|▋         | 15/210 [00:01<00:17, 11.45it/s]  8%|▊         | 17/210 [00:01<00:15, 12.45it/s]  9%|▉         | 19/210 [00:01<00:14, 13.58it/s] 10%|█         | 21/210 [00:01<00:12, 14.64it/s] 11%|█         | 23/210 [00:02<00:12, 15.37it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.35it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.06it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.94it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.99it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.79it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.50it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.41it/s] 19%|█▉        | 40/210 [00:03<00:10, 16.00it/s] 20%|██        | 42/210 [00:03<00:11, 14.99it/s] 21%|██▏       | 45/210 [00:03<00:10, 16.46it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.41it/s] 24%|██▍       | 51/210 [00:03<00:08, 17.96it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.55it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.47it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.23it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.57it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.02it/s] 30%|███       | 64/210 [00:04<00:08, 16.61it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.33it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.78it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.13it/s] 34%|███▍      | 72/210 [00:04<00:08, 15.90it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.30it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.57it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.24it/s] 39%|███▊      | 81/210 [00:05<00:08, 16.03it/s] 40%|████      | 84/210 [00:05<00:07, 16.83it/s] 41%|████▏     | 87/210 [00:05<00:06, 17.60it/s] 42%|████▏     | 89/210 [00:05<00:07, 17.15it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.71it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.34it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.95it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.46it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.02it/s] 50%|█████     | 105/210 [00:06<00:05, 19.12it/s] 51%|█████     | 107/210 [00:06<00:05, 18.35it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.18it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.64it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.69it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.50it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.49it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.58it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.21it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.95it/s] 60%|██████    | 126/210 [00:08<00:04, 17.35it/s] 61%|██████    | 128/210 [00:08<00:04, 16.97it/s] 62%|██████▏   | 130/210 [00:08<00:04, 16.06it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.38it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.80it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.38it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.70it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.59it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.37it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.13it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.96it/s] 70%|███████   | 148/210 [00:09<00:04, 15.16it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.11it/s] 72%|███████▏  | 152/210 [00:09<00:05, 11.58it/s] 74%|███████▍  | 155/210 [00:10<00:04, 13.71it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.21it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.13it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.52it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.74it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.63it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.12it/s] 80%|████████  | 169/210 [00:10<00:02, 15.62it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.29it/s] 82%|████████▏ | 173/210 [00:11<00:02, 17.20it/s] 83%|████████▎ | 175/210 [00:11<00:02, 16.62it/s] 84%|████████▍ | 177/210 [00:11<00:01, 16.74it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.22it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.69it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.63it/s] 89%|████████▊ | 186/210 [00:11<00:01, 16.85it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.37it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.88it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.87it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.55it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.96it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.51it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.19it/s] 97%|█████████▋| 204/210 [00:13<00:00, 17.06it/s] 98%|█████████▊| 206/210 [00:13<00:00, 16.98it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.72it/s]100%|██████████| 210/210 [00:13<00:00, 15.53it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1836   76]
 [ 145   36]]
tn=1836, fp=76, fn=145, tp=36
Number of predicted 0s: 1981
Number of actual 0s: 1912
Number of predicted 1s: 112
Number of actual 1s: 181
Precision: 0.32142857142857145
Accuracy: 0.8944099378881988
Recall: 0.19889502762430938
F1 score: 0.24573378839590446
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=8-step=1854.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:46,  1.97it/s]  1%|▏         | 3/210 [00:00<00:43,  4.72it/s]  2%|▏         | 5/210 [00:00<00:27,  7.55it/s]  3%|▎         | 7/210 [00:00<00:20,  9.86it/s]  4%|▍         | 9/210 [00:01<00:16, 12.03it/s]  5%|▌         | 11/210 [00:01<00:15, 12.72it/s]  6%|▌         | 13/210 [00:01<00:15, 12.52it/s]  7%|▋         | 15/210 [00:01<00:17, 11.25it/s]  8%|▊         | 17/210 [00:01<00:15, 12.29it/s]  9%|▉         | 19/210 [00:01<00:14, 13.52it/s] 10%|█         | 21/210 [00:01<00:12, 14.54it/s] 11%|█         | 23/210 [00:02<00:12, 15.34it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.31it/s] 13%|█▎        | 27/210 [00:02<00:11, 16.02it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.81it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.74it/s] 16%|█▌        | 34/210 [00:02<00:10, 17.49it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.28it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.19it/s] 19%|█▉        | 40/210 [00:03<00:10, 15.79it/s] 20%|██        | 42/210 [00:03<00:11, 14.92it/s] 21%|██▏       | 45/210 [00:03<00:10, 16.45it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.40it/s] 24%|██▍       | 51/210 [00:03<00:08, 17.90it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.51it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.44it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.25it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.57it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.03it/s] 30%|███       | 64/210 [00:04<00:08, 16.65it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.33it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.73it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.12it/s] 34%|███▍      | 72/210 [00:05<00:08, 15.86it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.21it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.42it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.13it/s] 39%|███▊      | 81/210 [00:05<00:08, 15.97it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.79it/s] 40%|████      | 85/210 [00:05<00:07, 17.20it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.22it/s] 42%|████▏     | 89/210 [00:06<00:07, 16.77it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.45it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.10it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.84it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.37it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.03it/s] 50%|█████     | 105/210 [00:06<00:05, 19.02it/s] 51%|█████     | 107/210 [00:06<00:05, 18.25it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.06it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.43it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.53it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.27it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.45it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.46it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.06it/s] 59%|█████▉    | 124/210 [00:08<00:05, 16.72it/s] 60%|██████    | 126/210 [00:08<00:04, 17.12it/s] 61%|██████    | 128/210 [00:08<00:04, 16.79it/s] 62%|██████▏   | 130/210 [00:08<00:05, 15.86it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.23it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.76it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.34it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.67it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.64it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.33it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.15it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.92it/s] 70%|███████   | 148/210 [00:09<00:04, 15.23it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.15it/s] 72%|███████▏  | 152/210 [00:10<00:05, 11.58it/s] 74%|███████▍  | 155/210 [00:10<00:04, 13.73it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.27it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.18it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.54it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.66it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.55it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.08it/s] 80%|████████  | 169/210 [00:11<00:02, 15.56it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.28it/s] 82%|████████▏ | 173/210 [00:11<00:02, 17.20it/s] 83%|████████▎ | 175/210 [00:11<00:02, 16.51it/s] 84%|████████▍ | 177/210 [00:11<00:01, 16.59it/s] 85%|████████▌ | 179/210 [00:11<00:01, 15.92it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.33it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.35it/s] 89%|████████▊ | 186/210 [00:12<00:01, 16.51it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.08it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.64it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.78it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.37it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.91it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.47it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.13it/s] 97%|█████████▋| 204/210 [00:13<00:00, 16.95it/s] 98%|█████████▊| 206/210 [00:13<00:00, 16.91it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.68it/s]100%|██████████| 210/210 [00:13<00:00, 15.44it/s]
/vol/bitbucket/es1519/myvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[[1843   69]
 [ 150   31]]
tn=1843, fp=69, fn=150, tp=31
Number of predicted 0s: 1993
Number of actual 0s: 1912
Number of predicted 1s: 100
Number of actual 1s: 181
Precision: 0.31
Accuracy: 0.8953655040611562
Recall: 0.1712707182320442
F1 score: 0.2206405693950178
Evaluating: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/saved/ROBERTA/lightning_logs/version_69942/checkpoints/converted/epoch=9-step=2060.ckpt
Dataset: /vol/bitbucket/es1519/NLPClassification_01/roberta_model/DontPatronizeMe/csv_files/dev.csv
Loading data: mode=TEST
  0%|          | 0/210 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/210 [00:00<01:43,  2.01it/s]  1%|▏         | 3/210 [00:00<00:44,  4.66it/s]  2%|▏         | 5/210 [00:00<00:27,  7.46it/s]  3%|▎         | 7/210 [00:00<00:20,  9.82it/s]  4%|▍         | 9/210 [00:01<00:16, 12.02it/s]  5%|▌         | 11/210 [00:01<00:15, 12.73it/s]  6%|▌         | 13/210 [00:01<00:15, 12.54it/s]  7%|▋         | 15/210 [00:01<00:17, 11.42it/s]  8%|▊         | 17/210 [00:01<00:15, 12.43it/s]  9%|▉         | 19/210 [00:01<00:14, 13.57it/s] 10%|█         | 21/210 [00:01<00:12, 14.59it/s] 11%|█         | 23/210 [00:02<00:12, 15.33it/s] 12%|█▏        | 25/210 [00:02<00:11, 16.29it/s] 13%|█▎        | 27/210 [00:02<00:11, 15.99it/s] 14%|█▍        | 29/210 [00:02<00:10, 16.78it/s] 15%|█▍        | 31/210 [00:02<00:11, 15.74it/s] 16%|█▌        | 34/210 [00:02<00:09, 17.70it/s] 17%|█▋        | 36/210 [00:02<00:10, 16.31it/s] 18%|█▊        | 38/210 [00:02<00:10, 16.18it/s] 19%|█▉        | 40/210 [00:03<00:10, 15.87it/s] 20%|██        | 42/210 [00:03<00:11, 14.85it/s] 21%|██▏       | 45/210 [00:03<00:10, 16.37it/s] 23%|██▎       | 48/210 [00:03<00:09, 17.32it/s] 24%|██▍       | 51/210 [00:03<00:08, 17.87it/s] 26%|██▌       | 54/210 [00:03<00:08, 18.56it/s] 27%|██▋       | 56/210 [00:03<00:08, 17.50it/s] 28%|██▊       | 58/210 [00:04<00:08, 17.22it/s] 29%|██▊       | 60/210 [00:04<00:08, 17.58it/s] 30%|██▉       | 62/210 [00:04<00:09, 16.03it/s] 30%|███       | 64/210 [00:04<00:08, 16.78it/s] 31%|███▏      | 66/210 [00:04<00:08, 17.51it/s] 32%|███▏      | 68/210 [00:04<00:09, 14.86it/s] 33%|███▎      | 70/210 [00:04<00:09, 15.00it/s] 34%|███▍      | 72/210 [00:05<00:08, 15.83it/s] 35%|███▌      | 74/210 [00:05<00:08, 16.25it/s] 36%|███▌      | 76/210 [00:05<00:08, 15.51it/s] 37%|███▋      | 78/210 [00:05<00:09, 14.18it/s] 39%|███▊      | 81/210 [00:05<00:08, 15.92it/s] 40%|███▉      | 83/210 [00:05<00:07, 16.80it/s] 40%|████      | 85/210 [00:05<00:07, 17.26it/s] 41%|████▏     | 87/210 [00:05<00:07, 17.33it/s] 42%|████▏     | 89/210 [00:06<00:07, 16.87it/s] 43%|████▎     | 91/210 [00:06<00:06, 17.59it/s] 44%|████▍     | 93/210 [00:06<00:06, 17.20it/s] 46%|████▌     | 96/210 [00:06<00:06, 18.93it/s] 47%|████▋     | 99/210 [00:06<00:06, 18.47it/s] 49%|████▊     | 102/210 [00:06<00:05, 20.13it/s] 50%|█████     | 105/210 [00:06<00:05, 19.15it/s] 51%|█████     | 107/210 [00:06<00:05, 18.28it/s] 52%|█████▏    | 109/210 [00:07<00:05, 18.08it/s] 53%|█████▎    | 111/210 [00:07<00:05, 17.51it/s] 54%|█████▍    | 114/210 [00:07<00:05, 17.59it/s] 55%|█████▌    | 116/210 [00:07<00:05, 17.31it/s] 56%|█████▌    | 118/210 [00:07<00:05, 17.47it/s] 57%|█████▋    | 120/210 [00:07<00:05, 16.44it/s] 58%|█████▊    | 122/210 [00:07<00:05, 17.05it/s] 59%|█████▉    | 124/210 [00:07<00:05, 16.67it/s] 60%|██████    | 126/210 [00:08<00:04, 17.09it/s] 61%|██████    | 128/210 [00:08<00:04, 16.77it/s] 62%|██████▏   | 130/210 [00:08<00:05, 15.87it/s] 63%|██████▎   | 132/210 [00:08<00:04, 16.18it/s] 64%|██████▍   | 134/210 [00:08<00:04, 15.71it/s] 65%|██████▍   | 136/210 [00:08<00:04, 16.25it/s] 66%|██████▌   | 138/210 [00:08<00:04, 15.60it/s] 67%|██████▋   | 140/210 [00:08<00:04, 16.49it/s] 68%|██████▊   | 142/210 [00:09<00:04, 15.27it/s] 69%|██████▊   | 144/210 [00:09<00:04, 14.00it/s] 70%|██████▉   | 146/210 [00:09<00:04, 14.84it/s] 70%|███████   | 148/210 [00:09<00:04, 15.07it/s] 71%|███████▏  | 150/210 [00:09<00:05, 11.06it/s] 72%|███████▏  | 152/210 [00:09<00:05, 11.53it/s] 74%|███████▍  | 155/210 [00:10<00:04, 13.71it/s] 75%|███████▍  | 157/210 [00:10<00:03, 14.19it/s] 76%|███████▌  | 159/210 [00:10<00:03, 15.12it/s] 77%|███████▋  | 161/210 [00:10<00:03, 15.43it/s] 78%|███████▊  | 163/210 [00:10<00:03, 14.57it/s] 79%|███████▊  | 165/210 [00:10<00:02, 15.52it/s] 80%|███████▉  | 167/210 [00:10<00:02, 16.02it/s] 80%|████████  | 169/210 [00:11<00:02, 15.66it/s] 81%|████████▏ | 171/210 [00:11<00:02, 16.46it/s] 83%|████████▎ | 174/210 [00:11<00:02, 16.69it/s] 84%|████████▍ | 177/210 [00:11<00:01, 17.03it/s] 85%|████████▌ | 179/210 [00:11<00:01, 16.43it/s] 86%|████████▌ | 181/210 [00:11<00:01, 16.67it/s] 88%|████████▊ | 184/210 [00:11<00:01, 16.44it/s] 89%|████████▊ | 186/210 [00:12<00:01, 16.56it/s] 90%|████████▉ | 188/210 [00:12<00:01, 16.12it/s] 91%|█████████ | 191/210 [00:12<00:01, 17.62it/s] 92%|█████████▏| 193/210 [00:12<00:01, 16.79it/s] 93%|█████████▎| 195/210 [00:12<00:00, 16.36it/s] 94%|█████████▍| 197/210 [00:12<00:00, 16.96it/s] 95%|█████████▍| 199/210 [00:12<00:00, 15.45it/s] 96%|█████████▌| 202/210 [00:12<00:00, 17.05it/s] 97%|█████████▋| 204/210 [00:13<00:00, 16.90it/s] 98%|█████████▊| 206/210 [00:13<00:00, 16.87it/s] 99%|█████████▉| 208/210 [00:13<00:00, 14.69it/s]100%|██████████| 210/210 [00:13<00:00, 15.44it/s]
[[1726  186]
 [ 131   50]]
tn=1726, fp=186, fn=131, tp=50
Number of predicted 0s: 1857
Number of actual 0s: 1912
Number of predicted 1s: 236
Number of actual 1s: 181
Precision: 0.211864406779661
Accuracy: 0.8485427615862399
Recall: 0.27624309392265195
F1 score: 0.23980815347721823
{'epoch=0-step=206.ckpt': {'accuracy': 0.19111323459149546, 'precision': 0.09528907922912205, 'recall': 0.9834254143646409, 'f1_score': 0.17374328940946804}, 'epoch=1-step=412.ckpt': {'accuracy': 0.8633540372670807, 'precision': 0.2463768115942029, 'recall': 0.281767955801105, 'f1_score': 0.26288659793814434}, 'epoch=2-step=618.ckpt': {'accuracy': 0.7673196368848543, 'precision': 0.20233463035019456, 'recall': 0.574585635359116, 'f1_score': 0.2992805755395683}, 'epoch=3-step=824.ckpt': {'accuracy': 0.8064978499761108, 'precision': 0.211340206185567, 'recall': 0.4530386740331492, 'f1_score': 0.28822495606326887}, 'epoch=4-step=1030.ckpt': {'accuracy': 0.8920210224558051, 'precision': 0.31092436974789917, 'recall': 0.20441988950276244, 'f1_score': 0.24666666666666665}, 'epoch=5-step=1236.ckpt': {'accuracy': 0.8905876731963689, 'precision': 0.3181818181818182, 'recall': 0.23204419889502761, 'f1_score': 0.268370607028754}, 'epoch=6-step=1442.ckpt': {'accuracy': 0.8991877687529861, 'precision': 0.3469387755102041, 'recall': 0.1878453038674033, 'f1_score': 0.2437275985663082}, 'epoch=7-step=1648.ckpt': {'accuracy': 0.8944099378881988, 'precision': 0.32142857142857145, 'recall': 0.19889502762430938, 'f1_score': 0.24573378839590446}, 'epoch=8-step=1854.ckpt': {'accuracy': 0.8953655040611562, 'precision': 0.31, 'recall': 0.1712707182320442, 'f1_score': 0.2206405693950178}, 'epoch=9-step=2060.ckpt': {'accuracy': 0.8485427615862399, 'precision': 0.211864406779661, 'recall': 0.27624309392265195, 'f1_score': 0.23980815347721823}}
