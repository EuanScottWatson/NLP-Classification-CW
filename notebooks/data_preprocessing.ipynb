{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeewookim/.conda/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = '../data/dontpatronizeme_pcl.tsv'\n",
    "\n",
    "# Train and Test data split data\n",
    "TRAIN_DATASET_IDX_PATH = '../data/train_test_split/train_semeval_parids-labels.csv'\n",
    "DEV_DATASET_IDX_PATH = '../data/train_test_split/dev_semeval_parids-labels.csv'\n",
    "\n",
    "SEED = 234\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "with open(FILE_PATH) as f:\n",
    "    for line in f.readlines()[4:]:\n",
    "        par_id=int(line.strip().split('\\t')[0])\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword= line.strip().split('\\t')[2]\n",
    "        country= line.strip().split('\\t')[3]\n",
    "        text = line.strip().split('\\t')[4]\n",
    "        label = int(line.strip().split('\\t')[-1])\n",
    "        if label == 0 or label == 1:\n",
    "            lbin = 0\n",
    "        else:\n",
    "            lbin = 1\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':text, \n",
    "            'label':lbin, \n",
    "            'orig_label':label\n",
    "            }\n",
    "            )\n",
    "\n",
    "df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train idx\n",
      " par_id     int64\n",
      "label     object\n",
      "dtype: object\n",
      "Dev idx\n",
      " par_id     int64\n",
      "label     object\n",
      "dtype: object\n",
      "Dataframe dtypes\n",
      " par_id         int64\n",
      "art_id        object\n",
      "keyword       object\n",
      "country       object\n",
      "text          object\n",
      "label          int64\n",
      "orig_label     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read indices of train and dev data\n",
    "train_idx_df = pd.read_csv(TRAIN_DATASET_IDX_PATH)\n",
    "dev_idx_df = pd.read_csv(DEV_DATASET_IDX_PATH)\n",
    "print(\"Train idx\\n\", train_idx_df.dtypes)\n",
    "print(\"Dev idx\\n\", dev_idx_df.dtypes)\n",
    "print(\"Dataframe dtypes\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We 're living in times of absolute insanity , as I 'm pretty sure most people are aware . For a while , waking up every day to check the news seemed to carry with it the same feeling of panic and dread that action heroes probably face when they 're trying to decide whether to cut the blue or green wire on a ticking bomb -- except the bomb 's instructions long ago burned in a fire and imminent catastrophe seems the likeliest outcome . It 's hard to stay that on-edge for that long , though , so it 's natural for people to become inured to this constant chaos , to slump into a malaise of hopelessness and pessimism .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 8375 | Train idxs: 8375\n",
      "Dev (Test) dataset size: 2094 | Train idxs: 2094\n",
      "Original Dataset Size: 10469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = df[df['par_id'].isin(train_idx_df['par_id'].values)]\n",
    "dev_df = df[df['par_id'].isin(dev_idx_df['par_id'].values)]\n",
    "\n",
    "\n",
    "def filter_features(df, features):\n",
    "    return df[features]\n",
    "\n",
    "# TODO: Add more features, like keyword, country, etc.\n",
    "# train_df = filter_features(train_df, ['par_id', 'text', 'label'])\n",
    "# dev_df = filter_features(dev_df, ['text', 'label'])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_df)} | Train idxs: {len(train_idx_df)}\")\n",
    "print(f\"Dev (Test) dataset size: {len(dev_df)} | Train idxs: {len(dev_idx_df)}\")\n",
    "print(f\"Original Dataset Size: {len(df)}\")\n",
    "assert len(df) == len(train_df) + len(dev_df)\n",
    "\n",
    "train_data, valid_data = train_test_split(train_df, test_size=0.2, random_state=SEED)\n",
    "test_data = dev_df\n",
    "\n",
    "# Save train, dev and test data\n",
    "train_data.to_csv('../data/train.tsv', sep='\\t', index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "valid_data.to_csv('../data/valid.tsv', sep='\\t', index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "test_data.to_csv('../data/dev.tsv', sep='\\t', index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'somewhere', \"'s\", '’ll', 'ten', \"'ve\", 'less', '‘ll', 'either', 'no', 'regarding', 'therein', 'whither', 'whoever', 'nobody', 'all', 'they', 'why', 'becoming', 'if', 'only', \"'re\", 'as', 'any', 'can', 'hereby', 'their', 'wherein', '’d', 'whether', 'also', 'our', 'empty', 'one', 'seems', 'somehow', 'top', 'fifty', 'not', 'thence', 'third', 'both', 'nor', 'neither', 'above', 'almost', 'themselves', 'show', '‘re', 'wherever', 'part', 'upon', 'many', 'former', 'whereas', '‘m', 'about', 'more', 'already', 'during', 'whose', 'put', 'has', 'whence', 'together', 'towards', 'me', 'never', \"n't\", 'another', 'and', 'forty', 'you', 'he', 'or', 'anyone', 'for', 'side', 'amount', 'until', 'an', 'see', 'thru', 'eight', 'were', 'what', 'last', 'such', 'further', 'via', '’s', 'else', 'hence', 'every', 're', 'ourselves', 'around', 'i', 'elsewhere', 'was', 'through', 'really', 'been', 'hers', \"'ll\", 'first', 'hereafter', 'do', 'doing', 'two', 'twelve', 'is', 'although', 'which', 'three', 'beyond', 'himself', 'take', 'down', 'due', 'these', 'made', 'where', 'by', 'done', 'them', 'whereby', 'within', 'quite', 'very', 'none', 'since', 'at', 'whereupon', \"'m\", 'thereupon', 'full', 'became', 'least', 'did', 'unless', 'herself', 'would', 'against', 'those', 'per', 'well', 'namely', 'everywhere', '‘d', 'because', 'here', 'the', 'front', 'now', 'however', 'under', 'over', 'in', 'am', 'beforehand', 'whole', 'than', 'latterly', 'yourself', 'latter', 'otherwise', 'does', 'herein', 'others', 'my', 'thus', 'six', 'nevertheless', 'too', 'enough', 'throughout', 'may', 'make', 'becomes', 'then', 'with', 'back', 'something', 'had', 'sometimes', 'just', \"'d\", 'off', 'n‘t', 'once', 'sometime', 'beside', '’m', 'should', 'though', 'after', 'eleven', 'to', 'toward', 'being', 'along', 'without', 'serious', 'nine', 'so', 'nothing', 'into', 'mostly', 'among', 'behind', 'hundred', 'noone', 'rather', 'hereupon', 'us', 'will', 'own', 'anyhow', 'that', 'except', '‘s', 'anything', 'afterwards', 'it', 'but', 'anyway', 'ca', 'please', 'indeed', 'bottom', 'cannot', 'therefore', 'fifteen', 'up', 'meanwhile', 'twenty', 'several', 'below', 'most', 'some', 'still', 'she', 'be', 'have', 'go', 'seeming', 'name', 'anywhere', 'say', 'everyone', 'formerly', '’ve', 'same', 'across', 'seem', 'four', 'next', 'each', 'of', 'when', 'his', 'thereafter', 'various', 'thereby', 'are', 'keep', '’re', 'whatever', 'yourselves', 'used', 'could', 'before', 'everything', 'a', 'alone', 'moreover', 'out', 'even', 'ever', 'myself', 'we', 'whom', 'call', 'must', 'seemed', 'amongst', 'few', 'other', 'besides', 'on', 'become', 'get', 'nowhere', 'yet', 'her', 'who', 'always', 'this', 'n’t', 'ours', 'perhaps', 'yours', 'whereafter', 'while', 'sixty', 'whenever', 'mine', 'there', 'from', 'move', 'give', 'using', 'how', 'someone', 'its', 'five', 'between', 'often', 'your', 'again', 'much', '‘ve', 'itself', 'onto', 'might'}\n"
     ]
    }
   ],
   "source": [
    "# # Parse dataframes to torchtext datasets\n",
    "# spacy_en = spacy.load('en_core_web_sm')\n",
    "# spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# print(spacy_stop_words)\n",
    "\n",
    "# def tokenizer(text): # create a custom tokenizer function\n",
    "#     return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# # TODO: Add more features such as country, keyword, etc.\n",
    "# text_field = Field(tokenize=tokenizer, lower=True, stop_words=spacy_stop_words)\n",
    "# label_field = Field(sequential=False, use_vocab=False) # we set sequential to false as we don't tokenise our labels\n",
    "\n",
    "\n",
    "# # order should match the columns order in our csv/tsv file\n",
    "# # if no processing was required, we set None\n",
    "# data_fields = [('Text', text_field), ('Label', label_field)]\n",
    "\n",
    "# # We will load our csv files into Dataset objects \n",
    "# train, val, test = TabularDataset.splits(\n",
    "#     path = '../data/',\n",
    "#     train = 'train.tsv',\n",
    "#     validation = 'valid.tsv',\n",
    "#     test = 'dev.tsv',\n",
    "#     format = 'tsv',\n",
    "#     fields = data_fields,\n",
    "#     skip_header = True\n",
    "# )\n",
    "\n",
    "# # possible dimensions for glove embeddings\n",
    "# EMBEDDING_DIM = [25, 50, 100, 200, 300]\n",
    "\n",
    "# text_field.build_vocab(train,max_size=25000, vectors=f\"glove.6B.{EMBEDDING_DIM[1]}d\")\n",
    "# label_field.build_vocab(train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "#         (train, val, test),\n",
    "#         batch_sizes= (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "#         sort_key=lambda x: len(x.Text), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83df55e12a4d26d0521b4f612eec2d51aeaade6148ccea2443671e6c8aaa0341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
